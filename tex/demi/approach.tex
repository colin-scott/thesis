% TODO(cs): in the overview part, don't mention Delta Debugging or DPOR?
% Instead refer to the abstractions?

Conceptually, one could find MCSes by enumerating and executing every possible
(valid, bounded) schedule containing the given external events. The globally minimal MCS would
then be the shortest sequence containing
the fewest external events that causes the safety violation. Unfortunately, the space of all
schedules is exponentially large, so executing all possible schedules is not feasible.
This leads us to our key challenge:

\begin{displayquote}
%\vspace{-0.5em}
{\em How can we maximize reduction of trace size within bounded time?}
%\vspace{-0.4em}
\end{displayquote}

% Since we would like to produce MCSes in useful time, it is acceptable that these
% algorithms occasionally fail to reduce the input trace significantly, however we require that exploration always finish
% in bounded time (specified by the user).

To find MCSes in reasonable time, we split schedule exploration into two
parts. We start by using delta debugging~\cite{Zeller:2002:SIF:506201.506206}
(shown in \S\ref{subsec:delta_debugging}), a minimization
algorithm similar to binary search, to prune extraneous external events. Delta debugging works by picking subsequences of external events, and checking whether
it is possible to trigger the violation with just those external
events starting from the initial configuration. We assume the user gives us a time budget, and we spread this budget evenly across each subsequence's exploration.

To check whether a particular subsequence of external events results in the safety violation,
we need to explore the space of possible interleavings of internal events and external events.
We use Dynamic Partial Order Reduction (`DPOR', shown in \S\ref{app:dpor}) to prune this schedule space by eliminating equivalent schedules (i.e. schedules
that differ only in the ordering of commutative
events~\cite{flanagan2005dynamic}). DPOR alone is insufficient though, since
there are still exponentially many non-commutative schedules to explore.
We therefore prioritize the order in which we explore the schedule space.

For any prioritization function we choose, an adversary could construct
the program under test to behave in a way that prevents our prioritization
from making any progress. In practice though,
programmers do not construct adversarial programs, and test orchestrators do not construct adversarial inputs. We choose our
prioritization order according to observations about how the programs we
care about behave in practice.

Our central observation is that if one schedule triggers a violation,
schedules that are similar in their causal structure should have a high probability
of also triggering the violation. Translating this intuition into
a prioritization function requires us to address our second
challenge:

\begin{displayquote}
%\vspace{-0.7em}
{\em How can we reason about the similarity or dissimilarity of two different
executions?}
%\vspace{-0.6em}
\end{displayquote}

We implement a hierarchy of $match$ functions that tell us whether messages
from the original execution correspond to the same logical message from the current execution.
We start our exploration with a single,
uniquely-defined schedule that closely resembles the original execution.
If this schedule does not reproduce the violation, we begin exploring
nearby schedules. % based on a notion of `edit-distance'.
We stop exploration once we have either
successfully found a schedule resulting in
the desired violation, or we have exhausted the time allocated for
checking that subsequence.

External event minimization ends once the system has successfully explored all subsequences generated by delta debugging. Limiting schedule exploration to a fixed
time budget allows minimization to finish in bounded time, albeit at the
expense of completeness (i.e., we may not return a perfectly minimal event sequence).
% TODO(cs): Are we sure this concept isn't soundness rather than completeness?

To further minimize execution length, we continue to use the same schedule exploration procedure
to minimize internal events once external event minimization has
completed. Internal event minimization continues until no more events
can be removed, or until the time budget for minimization as a whole is exhausted.

Thus, our strategy is to (i) pick subsequences with
delta debugging, (ii) explore the execution of
that subsequence with a modified version of DPOR, starting with a schedule that closely matches the
original, and then by exploring nearby schedules, and
(iii) once we have found a near-minimal MCS, we attempt
to minimize the number of internal events.
With this road map in mind, below we describe our
minimization approach in
greater detail.

\subsection{Choosing Subsequences of External Events}

We model the task of minimizing a sequence of external events $E$ that causes an invariant violation
as a function $ExtMin$ that repeatedly removes parts of $E$ and invokes an
oracle (defined in~\S\ref{subsec:exploration_strategies})
to check whether the resulting subsequence, $E'$, still triggers the violation. If $E'$ triggers the
violation, then we can assume that the parts of $E$ removed to produce $E'$ are not required
for producing the violation and are thus not a part of the MCS.

$ExtMin$ can be trivially implemented by removing events one at a time
from $E$, invoking the oracle at each iteration. However, this would require that we check
$O(|E|)$ subsequences to determine whether each triggers the violation.
Checking a subsequence
is expensive, since it may require exploring a large set of event schedules.
We therefore apply
delta debugging~\cite{Zeller:1999:YMP:318773.318946,Zeller:2002:SIF:506201.506206}, an algorithm similar to
binary search, to achieve $O(log(|E|))$ average case runtime (worst case
$O(|E|)$). The delta debugging algorithm we use is shown in
\S\ref{subsec:delta_debugging}.

Efficient implementations of $ExtMin$ should not waste time trying to execute invalid
(non-sensical) external event subsequences. We maintain validity by
ensuring that forced restarts are always preceded by a start
event for that process, and by assuming that external
messages are independent of each other, i.e., we do not currently support external messages that, when removed, cause some
other external event to become invalid. One could support minimization of
dependent external messages by either requiring the user to provide a grammar,
or by employing the $O(|E|^2)$ version of delta debugging that considers
complements~\cite{Zeller:2002:SIF:506201.506206}.

\subsection{Checking External Event Subsequences}

Whenever delta debugging selects an external event sequence $E'$, we need to check
whether $E'$ can result in the invariant violation. This requires that we enumerate
and check all schedules that contain $E'$ as a subsequence.
Since the number of possible schedules is exponential in the number of events,
pruning this schedule space is essential to finishing in a timely manner.

As others have observed~\cite{godefroid1995partial}, many events occurring in a schedule are {\em commutative}, i.e., the system arrives at the same
configuration regardless of the order events are applied. For example, consider two events $e_1$ and $e_2$, where
$e_1$ is a message from process $a$ being delivered to
process $c$, and $e_2$ is a message from process $b$
being delivered to process $d$. Assume that both $e_1$ and $e_2$ are co-enabled, meaning they are both pending at the same time and can be executed in either order. Since
the events affect a disjoint set of nodes ($e_1$ changes the state at $c$, while $e_2$ changes the state at $d$), executing $e_1$ before $e_2$ causes the system to arrive at the same state it would arrive at if we had instead executed $e_2$ before $e_1$.
$e_1$ and $e_2$ are therefore commutative. This example
illustrates a form of commutativity captured by the happens-before relation~\cite{Lamport:1978:TCO:359545.359563}: two message delivery events
$a$ and $b$ are commutative if they are concurrent, i.e. $a \not\rightarrow b$
and $b \not\rightarrow a$, and they affect a disjoint set of nodes.\footnote{
Stronger forms of
commutativity may hold if events cannot possibly be causally unrelated to each other, for
example when a distributed algorithm is stateless. Inferring such cases of
commutativity would require understanding of application semantics; in
contrast, happens-before commutativity is independent of the application.}

Partial order reduction (POR)~\cite{godefroid1995partial,flanagan2005dynamic} is a well-studied technique for pruning commutative schedules from the search space. In the above example, given two schedules that only differ in the order
in which $e_1$ and $e_2$ appear, POR would only explore one schedule. Dynamic POR (DPOR)~\cite{flanagan2005dynamic} is a
refinement of POR (shown in \S\ref{app:dpor}): at each
step, it picks a pending message to deliver, dynamically computes which
other pending events are not concurrent with the message it just delivered,
and sets backtrack points for each of these, which it will later use (when
exploring other non-equivalent schedules) to try delivering the pending
messages in place of the message that was just delivered.

Even when using DPOR, the task of enumerating all possible schedules containing $E$ as a
subsequence remains intractable. Moreover, others have found that na\"ive DPOR
gets stuck exploring a small portion of the schedule space because of its
depth-first exploration order~\cite{lin2009modist}.
%, and require that a large number of schedules be explored
%before an invariant violation is found.
We address this problem in two ways: first, as mentioned before, we limit
$ExtMin$ so it spreads its fixed time budget roughly evenly across
checking whether each particular subsequence of external events reproduces the invariant violation. It does this by
restricting DPOR to exploring a fixed number of schedules before
giving up and declaring that an external event sequence does not produce the violation. Second, to maximize the probability that
invariant violations are discovered quickly while exploring a fixed number of
schedules, we employ a set of schedule exploration strategies to guide
DPOR's exploration, which we describe next.

\subsubsection{Schedule Exploration Strategies}
\label{subsec:exploration_strategies}

We guide schedule exploration by manipulating two degrees of freedom within
DPOR: (i) we prescribe which pending events DPOR initially
executes, and (ii) we prioritize the order backtrack points are explored in.
In its original form, DPOR only performs
depth-first search starting from an arbitrary initial schedule, because it was designed to be {\em
stateless} so that it can run indefinitely in order to find as
many bugs as possible. Unlike the traditional use case, our goal is to minimize a known bug in a
timely manner. By keeping some state tracking the schedules we have already explored, we can pick
backtrack points in a prioritized (rather than depth-first) order without exploring redundant schedules.

% TODO(cs): recap that Min invokes the scheduling strategy?
A scheduling strategy implements a backtrack prioritization order.
%We model scheduling strategies as functions $f(E') \Rightarrow \tau'\;\vert\;\bot$
%that return the smallest schedule they can find ($\tau'$) containing a given external event
%subsequence $E'$ that triggers the violation $\overline{P}$. If a scheduling strategy cannot find a
%reproducing schedule within its time budget, it returns a special marker $\bot$.
Scheduling strategies return the first violation-reproducing schedule
(containing a given external event subsequence $E'$) they
find within their time budget. If a scheduling strategy cannot find a
reproducing schedule within its time budget, it returns a special marker $\bot$.
We design our key strategy (shown in
Algorithm~\ref{fig:alg_overview})
with the following observations in mind:

\noindent{\textbf{Observation \#1: Stay close to the original execution.}} The original schedule
provides us with a
`guide' for how we can lead the program down a code path that makes progress towards
entering the same unsafe state. By choosing modified schedules that have causal
structures that are close to the original schedule, we should have high
probability of retriggering the violation.

We realize this observation by starting our exploration with a single, uniquely
defined schedule for each external event subsequence: deliver only messages
whose source, destination, and contents
`match' (described in detail below) those in the original execution, in the exact same order
that they appeared in the original execution. If an internal message from the
original execution is not pending (i.e. sent previously by some actor) at the
point that internal message should be delivered, we skip over it and
move to the next message from the original execution. Similarly, we
ignore any pending messages that do not match any events delivered in the original
execution. In the case where multiple pending messages match, it does not
matter which we choose (see Observation \#2).
We show an example initial
schedule in Figure~\ref{fig:matching}.

\begin{figure}[tb!]
    \centering
    \includegraphics[width=3.15in]{../diagrams/matching_schedules.pdf}
    %\vspace{-1em}
    \caption{\label{fig:matching} Example schedules. External
    message deliveries are shown in red, internal message deliveries in green. Pending
    messages, source addresses, and destination addresses are not shown. The `B' message
    becomes absent when exploring the first subsequence of external
    events. We choose an initial schedule that is close to the original,
    except for the masked `seq' field. The violation is not triggered after
    the initial schedule (depicted as $\PASS$), so we next match messages by type,
    allowing us to deliver pending messages with smaller `Term' numbers. The violation is still not triggered, so we continue exploring.}
    %\vspace{-1.5em}
\end{figure}


\noindent{\textbf{Matching Messages.}} A function $match$ determines whether a
pending message from a modified execution logically corresponds to a
message delivered in the original execution. The simplest way to implement
$match$ is to
check equality of the source, the destination, and all bytes of the message
contents. Recall though that we are executing a
{\em subsequence} of the original external events.
In the modified execution the contents of many of the
internal messages will likely change relative to message contents from the original execution. Consider, for example, sequence numbers
that increment once for every message a process receives (shown as the `seq` field in Figure~\ref{fig:matching}). These differences in
message contents prevent simple bitwise equality from finding
many matches.

\noindent{\textbf{Observation \#2: Data independence.}} Often, altered message
contents such as differing sequence numbers do {\em not} affect the behavior of the
program, at least with respect to whether the program will reach the
unsafe state. Formally, this property is known as
`data-independence', meaning that the values of some message contents
do not affect the system's control-flow~\cite{shacham2014verifying,wolper}.

Some types of message fields obviously exhibit
data-independence. Consider authentication cookies. Distributed systems commonly
use cookies to track which requests are tied to which users. However, the
value of the cookie is not in itself relevant to how the distributed system
behaves, as long as each user attaches a consistent value to their messages. This means that two executions
with exactly the same users and exactly the same messages except for the
cookie fields can be considered equivalent.

To leverage data independence, application developers can (optionally) supply us
with a `message fingerprint' function,\footnote{It may be possible to extract
message fingerprints automatically using program analysis or experimentation~\cite{diffy}.
Nonetheless, manually defining fingerprints does not require much effort (see
Table~\ref{tab:instrumentation}). Without a fingerprint function, we default
to matching on message type (Observation \#3).}
which given a message returns a string that depends on the relevant parts of the message,
without considering fields that should be ignored when checking if two message instances from
different executions refer to the same logical message.
An example fingerprint function might ignore sequence numbers and authentication cookies, but
concatenate the other fields of messages.
Message fingerprints are useful both as a way of mitigating non-determinism, and as a way of
reducing the number of schedules the scheduling strategy needs to explore (by drawing an equivalence relation
between all schedules that only differ in their masked fields).
We do not require strict data-independence in the formal
sense~\cite{shacham2014verifying}; the fields the user-defined fingerprint
function masks over may in practice affect the control flow of the program,
which is generally acceptable because we simply use this as a strategy to guide the
choice of schedules, and can later fall back to exploring all schedules if we
have enough remaining time budget.

% Currently, we encode message fingerprint functions
% manually before testing, by examining the message protocol specification for each distributed
% system we test. However, it could be possible to analyze the control-flow of
% the program under test to generate message fingerprint functions in a sound
% manner.

% TODO(cs): description of what fields we mask off for fingerprints?
% TODO(cs): one lesson we learned: need to be careful when choosing
% fingerprints! If you're too general, you won't end up ever finding the bug,
% even on unmodified executions.
% TODO(cs): George's question: are fingerprints as unique symbols equivalent
% to content masks?

We combine observations \#1 and \#2 to pick a single, unique
schedule as the initial execution, defined by selecting pending events in
the modified execution that $match$ the original execution.
This stage corresponds to the first two lines of \textproc{TEST} in
Algorithm~\ref{fig:alg_overview}. We
show an example initial schedule in Figure~\ref{fig:matching}.

\eat{
\noindent{\textbf{First Minimization Pass.}} As a first minimization pass, we combine
observations \#1 and \#2: we run $ExtMin$ to completion, and for each external
event subsequence chosen by $ExtMin$, we test the single, uniquely defined
schedule given by observation \#2. \colin{Currently incongruent with the flow..}.
% After $ExtMin$ completes, we then shift our
%efforts to minimizing internal events: we attempt to prune each remaining internal event (from the shortest schedule containing $E'$)
%by {\em not} delivering that message, testing the same uniquely defined schedule
%for the remaining events, and checking whether the invariant violation still
%occurs. % Note that the first pass does not explore any backtrack points; it
% only chooses initial traces for DPOR to test.
}
% STSSched only tests a single schedule for each external event subsequence chosen by delta
% debugging. We can do better by exploring multiple schedules per subsequence.
\eat{ % ---- background thread -------
\begin{algorithm}
\caption{\label{fig:intercept_messages} A background thread intercepts all
messages.}
\begin{algorithmic}
\State pending $\gets [\,]$ \Comment{Pending internal events}
\Procedure{interceptMessages}{}
\While{true}
\ForAll{message sent}
\State pending $\gets$ pending + message
\EndFor
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}
}

\eat{ % ---- notation ----
\begin{figure*}
\footnotesize
\begin{boxedminipage}{\textwidth}
\begin{center}
\begin{align*}
E &= \left\{ e_1, e_2, \ldots, e_n  \right\} && \text{External events where $e_j$ is an external event}  \\
I &= \left\{ i_1, i_2, \ldots, i_n \right\} && \text{Internal events where $i_j$ is an internal events} \\
\tau &= e_1\rightarrow i_1\rightarrow \ldots e_2\rightarrow i_m \ldots && \text{Schedule: A particular execution}  \\
\tau^{\circ} & \text{ } && \text{The original schedule} \\
ready &: E \Rightarrow \left\{true, false\right\} && \text{Returns whether a
given external event's precondition holds} \\
V & \text{ } && \text{Invariant violation, a safety condition}  \\
f_{p} &: \mathcal P \left({E}\right) \Rightarrow \left\{true, false\right\} &&
\text{Test oracle: returns }\, \exists_{\tau} \, \text{s.t.} \, \tau \, \text{triggers V} \\
Min &: \left\{E\right\} \Rightarrow \mathcal P \left({E}\right) && \text{Minimization procedure. Repeatedly invokes}\, f_{p}
\end{align*}
\end{center}
\end{boxedminipage}
\caption{Notation}
\label{fig:notation}
\end{figure*}
}   % ----

\begin{algorithm}[tb!]
\caption{{\label{fig:alg_overview} Pseudocode for schedule exploration.
\textproc{TEST} is invoked once per external event subsequence $E'$.
We elide the details of DPOR for clarity (see \S\ref{app:dpor} for a complete
description). $\tau$ denotes the
original schedule; b.counterpart denotes the message delivery
that was delivered instead of b (variable m in the elif branch of \textproc{STSSched}); b.predecessors and b.successors denote the events occuring before and after b when b was set ($\tau''$[0..i] and $\tau''$[i+1...$\tau''$.length] in \textproc{STSSched}).}}
\begin{algorithmic}
\State{backtracks $\gets$ $\{\}$}
\Procedure{test}{$E'$}
\State{\Call{STSSched}{$E'$,$\tau$}}
\If{execution reproduced $\DFAIL$}
\Return{$\DFAIL$}
\EndIf
\While{$\exists_{\text{b} \in \text{backtracks.}}$b.type$=$b.counterpart.type $\wedge$ \\
\hskip3em\relax b.fingerprint $\neq$ b.counterpart.fingerprint $\wedge$ \\
\hskip3em\relax time budget for $E'$ not yet expired}

  \State{reinitialize system, remove b from backtracks}
  \State{prefix $\gets$ b.predecessors $+\; [$ b $]$}
  \If{prefix (or superstring) already executed}
    \State{{\textbf{continue}}}
  \EndIf
  \State{\Call{STSSched}{$E'$,prefix + b.successors}}
  \If{execution reproduced $\DFAIL$}
  \Return{$\DFAIL$}
  \EndIf
\EndWhile
%\Return{$\PASS$}
\State{{\textbf{return}} $\PASS$}
\EndProcedure

% Play prefix, before backtrack point, plus wildcards...
% TODO(cs): think about whether absent wildcards would cause us to redundantly
% explore schedules.

\Procedure{STSSched}{$E'$,$\tau'$}
\State{$\tau'' \gets \tau'$.remove $\{e \;|\; e \; \text{is external and }e \not\in E' \}$}
\For{i from 0 to $\tau''$.length}
\If{$\tau''$[i] is external}
  \State{inject $\tau''$[i]}
\ElsIf{$\exists_{\text{m}\in\text{pending}.}$ m.fingerprint $= \tau''$[i].fingerprint}
  \State{deliver m, remove m from pending}
  \For{m$'$ $\in$ pending}
  \If{$\neg\,$commute$($m,m$')$}
    \State{backtracks $\gets$ backtracks $\cup\;\{$m$'\}$}
  \EndIf
  \EndFor
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

%\setlength{\textfloatsep}{1pt}% Remove \textfloatsep

% \noindent{\bf Challenge: History-Dependent Message Contents.} The first
% minimization pass can be remarkably effective. However, we find
% that it suffers from a key limitation that prevents it from pruning some
% common types of extraneous events: it handles message contents that depend on previous
% events poorly.
\noindent{\textbf{Challenge: history-dependent message contents.}} This initial schedule
can be remarkably effective, as demonstrated by the fact that minimization
often produces significant reduction
even when we limit it to exploring this single schedule per external event subsequence. However, we find
that without exploring additional schedules, the MCSes we find still
contain extraneous events: when message contents depend on previous
events, and the messages delivered in the original execution contained
contents that depended on a large number of prior events, the initial schedule
will remain inflated because it never includes ``unexpected'' pending messages
that were not delivered in the original execution yet have contents that depend on fewer prior
events.

% TODO(cs): cut or shorten second example if we need space
To illustrate, let us consider two example faulty executions of the Raft
consensus protocol. The first execution was problematic because all Raft messages contain logical clocks (``Term numbers'') that
indicate which epoch the messages belong to. The logical clocks are
incremented every time there is a new leader election cycle. These logical
clocks {\em cannot} be masked over by the message fingerprint, since they play
an important role in determining the control flow of the program.

% STSSched
% performs well at allowing delta debugging to minimize external
% events, as we show in~\S\ref{dsec:evaluation}. However, STSSched
% does not perform particularly well when used to try to minimize
% internal events (where, instead of choosing subsequences of external events,
% we try to ignore certain internal message deliveries and keep the others in
% the same order). There are also some cases where STSSched fails to remove
% external events (i.e. find failing executions for delta debugging) when they
% are clearly removable.

% TODO(cs): adopt the bug trace notation from the SAMC paper? Much more concise and readable.

In the original faulty execution, the
safety violation happened to occur at a point where
logical clocks had high values, i.e. many leader election cycles had already taken
place. We knew however that
most of the leader election cycles in the beginning of the execution were not
necessary to trigger the safety violation. Minimization restricted to only
the initial schedule was {\em not} able to remove the earlier
leader election cycles, though we would have been able to if we had instead delivered
other pending messages with small term numbers.

% An example: Suppose we have a three-node Raft cluster. There are two cases where a Raft node will increment
% its ``Term number'', a sort of logical clock: (i) if it receives an election timer,
%  it will increment its Term and attempt to get itself elected, and (ii) if it receives a
%  message from another node whose Term number is higher, it will set its own Term number to that of its peer.
%
% Suppose we have a hypothetical bug where a node will crash as soon as it is elected.
%
% In our hypothetical execution, the following events occur (node3 need not participate): (i) node1 receives election timer, and
% sends RequestVote(Term(1)) to others; (ii) node2 receives receives RequestVote(Term(1)), sets its Term to 1, and
% attempts to respond with Vote(Term(1)); (iii) node2 receives an election timer before its response is delivered, and
% sends RequestVote(Term(2)) to others; (iv) node1 receives receives  RequestVote(Term(2)), sets its Term to 2 and attempts to respond
% (v) node1 receives election timer before its response is delivered, and sends RequestVote(Term(3)) to others...
% eventually, (X) node2 receives RequestVote(Term(N)), sets its Term to N and responds with Vote(Term(N)), and (XI)
% node1 receives Vote(Term(N)) from node2 and crashes.
% \george{I feel that this amount of detail is not needed to explain the idea, or perhaps it can be explained in a shorter form?
% Also, the reader may wonder why are these logical clocks not something that the fingerprint can take care of.
% I think it is because they are important for the correctness of the protocol, yet they carry over too much
% of the actual history of the communication (which relates to the issue of 'Batched Message Contents'}


% TODO(cs): potential issue with example: it depends on UDP, i.e. this technique
% is not applicable to systems that assume TCP as their underlying transport protocol.
The second execution was problematic because of {\em batching}. In
Raft, the leader receives client commands, and after receiving each
command, it replicates it to the other
cluster members by sending them `AppendEntries' messages. When the leader receives multiple client commands
before it has successfully replicated them all, it batches them
into a single AppendEntries message. % for each cluster member.
Again, client commands cannot be masked over by the fingerprint function, and
because AppendEntries are internal messages, we cannot shrink their
contents.

We knew that the safety violation
could be triggered with only one client command.
Yet minimization restricted to only the initial schedule was unable to prune many client
commands, because in the original faulty execution AppendEntries
messages with large batch contents were delivered before pending
AppendEntries messages with small batch contents.

These examples motivated our next observations:

\noindent{\textbf{Observation \#3: Coarsen message matching.}} We would like to
stay close to the original execution (per observation \#1), yet the previous
examples show that we should not restrict ourselves to schedules that only
match according to the user-defined message fingerprints from the original execution. We
can achieve both these goals by considering a more coarse-grained $match$
function: the {\em type} of pending messages. By `type', we mean the
language-level type tag of the message object, which is
available to the RPC layer at runtime before the message is converted to bits on the wire.

We choose the next schedules to explore by looking for pending messages whose
{\em types} (not contents) match those in the original execution, in the exact same order
that they appeared in the original execution. We show an example in
Figure~\ref{fig:matching}, where any pending message of type `A' with the same
source and destination as the original messages would match. When searching for candidate schedules, if there
are no pending messages that match the type of the message that was delivered
at that step in the original execution, we skip to the next step.
Similarly, we
ignore any pending messages that do not match the corresponding type of the
messages from the original
execution. This leaves one remaining issue: how we handle cases where multiple
pending messages match the corresponding original message's type.

% TODO(cs): this potentially violates the TCP (FIFO) scheduling discipline.
% TODO(cs): this obersvation isn't really distinct from Observation #3...
\noindent{\textbf{Observation \#4: Prioritize backtrack points that resolve match
ambiguities.}} When there are multiple pending messages that match,
we initially only pick one. DPOR (eventually) sets backtrack points for all other co-enabled
dependent events (regardless of type or message contents).
Of all these backtrack points, those that match the type of the corresponding message
from the original trace should be most fruitful, because they keep the execution close to the causal structure of the
original schedule except for small ambiguities in message contents.

We show the pseudocode implementing Observation \#3 and Observation \#4 as the while loop in Algorithm~\ref{fig:alg_overview}.
Whenever we find a backtrack point (pending message) that matches the type but not the
fingerprint of an original delivery event from $\tau$, we replace the original
delivery with the backtrack's pending message, and execute the events before
and after the backtrack point as before. We also track which schedules we have executed in the past
to avoid redundant exploration.

Backtracking allow us to eventually explore all combinations of pending messages that
match by type. Note here that we do not ignore the user-defined message
fingerprint function: we only prioritize backtrack points for pending messages that
have the same type {\em and} that differ in their message fingerprints.


%\colin{TODO(cs): Arvind's "Provenance Fingerprint" as a mechanism, and also as a way of introducing DepGraph}
% DepGraph writeup here: https://docs.google.com/document/d/1D0PuYRvJG6ucdHKBwRT3yXUIUvYz2rLLo9fOY79ks0w/edit

% TODO(cs): also describe our domain specific minimization routine (replacement for delta debugging), based on clustering logical clocks.
% As usual, we can
% separate the minimization problem into two subproblems:
%   - Choosing how to “chop” up the original execution into subsequences to be replayed
%   - Performing the replay for each chosen subsequence
% The first problem often benefits from domain-specific knowledge. I believe the second can be solved in a general way.

% Choosing subsequences:
%
% This part is domain-specific. We might just run delta debugging, but I believe the following should be quite a bit more efficient.
%
% Proposed approach:
%
% Have the user give us a function:
%
% extractLogicalClock: Message -> Integer
%
% Given this function, we cluster messages according to their logical clocks. Then, we iteratively try to remove clusters early in the trace.
%
% We are however left with one remaining challenge: we don’t know how many timers to deliver (each of which causes the Term number to increment).
%
% To deal with this challenge, we try experimentation rather than explicitly reasoning about the actor’s states. Specifically, for each cluster we try to remove, we start by including all timers, and iteratively try to remove each timer one at a time. If we ever find an execution where a timer was not present, we never include that timer again.
%
% Overall, the complexity of this approach O(t * k), where t is the number of timers, and k is the number of logical clock clusters.

% TODO(cs): Fallback, once we're done with Wildcard DPOR. Yet, we don't actually
% fallback to this at the moment... just a pending implementation issue.
% ---------------------- \subsection{Edit-Distance DPOR.} -------------------
% Finally, once we have applied
% STSSched and WMB, we fall back to attempting to enumerate as many schedules we
% can in the remaining time budget. Something something edit distance.

\noindent{\textbf{Minimizing internal events.}} Once delta debugging over external
events has completed,
we attempt to further reduce the smallest reproducing schedule found so far.
Here we apply delta debugging to internal events: for each subsequence of
internal events chosen by delta debugging, we (i) mark those messages so that they
are left pending and never delivered, and (ii) apply the same scheduling
strategies described above for the remaining events to check whether the
violation is still triggered.
Internal event minimization continues until there is no more minimization to
be performed, or until the time budget for minimization as a whole is exhausted.

\noindent{\textbf{Observation \#5: Shrink external message contents whenever
possible.}} Our last observation is that the contents of external messages
can affect execution length; because the test environment crafts these
messages, it should minimize their contents whenever
possible.

A prominent example is akka-raft's bootstrapping messages. akka-raft
processes do not initially know which other processes are part of the cluster.
They instead wait to receive an external bootstrapping message that informs
them of the identities of all other processes. The contents of the bootstrapping
messages (the processes in the cluster) determine {\em quorum
size}: how many acknowledgments are needed to reach consensus, and hence
how many messages need to be delivered. If the application developer
provides us with a function for separating the components of such message
contents, we can minimize their
contents by iteratively removing elements, and checking to see if the violation is still triggerable until
no single remaining element can be removed.

\noindent{\textbf{Recap.}} In summary, we first apply delta debugging ($ExtMin$) to prune external events.
To check each external event subsequence chosen by delta debugging, we use a stateful version of DPOR.
We first try exploring a uniquely defined schedule that closely matches the original execution.
We leverage data independence by applying a
user-defined message fingerprint function that masks over certain message
contents. To overcome inflation due to
history-dependent message contents, we explore subsequent schedules by
choosing backtrack points according to a more coarse-grained match function: the
types of messages. We spend the remaining time budget attempting to minimize internal events, and wherever
possible, we seek to shrink external message contents.

\subsection{Comparison to Prior Work}

We made observations \#1 and \#2 in Chapter~\ref{sec:sts}. In this
Chapter, we adapt observations \#1 and \#2 to determine the first schedule we
explore for each external event subsequence (the first two lines of \textproc{TEST}). We refer
to the scheduling strategy defined by these two observations as `STSSched', named
after the `STS' system (\S\ref{sec:sts}).

STSSched only prescribes a single schedule per external event subsequence
chosen by delta debugging. In
this work we systematically explore multiple schedules using the DPOR
framework. We guide DPOR to explore schedules in a prioritized order based on similarity to the original execution
(observations \#3 and \#4, shown as the while loop in \textproc{TEST}).
We refer to the scheduling strategy used to prioritize subsequent schedules as `TFB' (Type Fingerprints with Backtracks).
We also minimize internal events, and shrink external message contents.

% ---------------------------- \subsection{Insights} --------------------

% Frame these insights in terms of adversaries:
%  - An adversary might choose a program P that behaves in a way that we won't
%    deal with well
%  - Stronger statement: given a fixed program P, an adversary might choose
%    external events E that we won't deal with well
% i.e., "there will always be counterexamples"
%
% But our key insight is that the world is not adversarial. We start by
% explaining exactly what we think are the bounds on the adversarial nature of
% programs, then in the evaluation section we evaluate how well our model of
% adversarielness matches up with reality.
%
% Now, I think it would be nice if our algorithm could achieve completeness
% *within the bounds* of our model of adversarialness, assuming we let it run
% long enough.

\eat{
\george{Not sure what is the plan with this. It feels more like a bunch of notes. If these get turned into text, drop the enumeration format.}
\begin{enumerate}
\item Systems often repeatedly return to quiescent states, where state from before
the quiescence is largely forgotten $\rightarrow$ rather than blindly choosing
subsequences as in delta debugging, try removing each cluster of events
delineated by a quiescent period

\item When replaying a given subsequence, the causal structure of remaining events
from the original execution provides an important hint for how to retrigger
the violation $\rightarrow$ attempt to stay as close to the original causal structure as
possible [focus on backtrack points that do not change the edit distance of
the causal structure much]

\item On the other hand, the application's control flow is often unaffected by
changes to certain parts of the message contents [e.g. sequence numbers]
$\rightarrow$
ignore backtrack points that only involve changes to message contents that the
developer has told us are irrelevant [``data-independence''~\cite{shacham2014verifying}]\george{Can use fingerprint terminology}

\item ``It's good to take random jumps around the schedule space rather than getting
stuck exploring one corner of the space~\cite{lin2009modist}'' $\rightarrow$ Enforce fair allocation of the
time budget across each possible subsequence

\item The application often assumes message scheduling guarantees below the RPC
layer, e.g. TCP instead of UDP $\rightarrow$ it's useless to explore schedules that
violate the guarantees assumed by the application.
\end{enumerate}
}

% TODO(cs): Optimization that we currently apply but haven't written about: inferring
% (some) of the internal messages we know are going to be absent in the
% subsequence. In particular, if we pruned a "Start" event for a given
% actor, we know that all messages destined to or coming from that actor
% will not occur in the subsequence execution.
% TODO(cs): discussion of existing approaches?
% TODO(cs): edit-distance as a way of avoiding unrealistic bugs (or rather,
% unrealistic reproductions of bugs)
% TODO(cs): Earlier idea: building a model of the state machine before
% minimizing, a la synoptic.
% TODO(cs): IDEA for a scheduling strategy: simply await quiescence between each external event
% Insight: we enforce linearizability, i.e. we have removed concurrency.
% TODO(cs): IDEA for a minimization strategy: remove events from right to left
% rather than left to right. The intuition is the following: since we control
% (most-of) the sources of non-determinism, we're guaranteed that if we replay a
% prefix of events exactly, the system should end up in the same state. So, by
% removing right-to-left, we have perturbation to worry about; we only need to
% examine the events in the tail.
% TODO(cs): Don't think of DPOR as a procedure we invoke. Instead think of it
% as a continuation. We first tell it: explore up to edit-distance=k. Call it
% on a bunch of subsequences chosen by delta debugging. Then, start delta
% debugging over again with edit-distance = k+1. Have DPOR remember what it
% explored before, and now only explore edit-distance =k+1.
% TODO(cs): optimiztion for Min: have DPOR provide `hints` about causality.
% Then have DDmin think about the external events as DAGs instead of a linear
% sequence. Don't just split arbitrarily, split along DAG lines. The DAGs are
% generated as `hints` based on lack of causality observed during DPOR runs.
% TODO(cs): optimization for Min: make sure we don't ever explore the same
% prefix (or powerset!), across all invocations of f_p. (A la, Vjeko's observation about DPOR
% subsuming minimization)
% TODO(cs): could optimize internal minimization (of all kinds, one-at-a-time, ddmin, fung clocks) by looking at DepGraph, not % bothering removing things we know are not going to show up.
% TODO(cs): Other heuristics (brainstorming):
% https://docs.google.com/document/d/13fEZB1EpMJZe_NUrn8DJqu0jWOL-0ha7h5rQhionHHQ/edit

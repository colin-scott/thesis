
\section{Delta Debugging}
\label{app:ddmin}

We show the Delta Debugging simplification
algorithm~\cite{Zeller:1999:YMP:318773.318946} we use in Figure~\ref{fig:ddmin}, and an
example execution of Delta Debugging in Figure~\ref{fig:ddmin_example}.
An updated version of the ddmin simplification algorithm appeared
in~\cite{Zeller:2002:SIF:506201.506206}. We use the simpler version of ddmin
(which is equivalent to the version ddmin
from~\cite{Zeller:2002:SIF:506201.506206}, except that it does not consider
complements) because we ensure that each subsequence
of external events is consistent (semantically valid), and therefore are still
guarenteed to find a 1-minimal output without needing
to consider complements.

\begin{table}[b]
\centering
\footnotesize
\begin{tabular}{l|llllllll|l}
\hline
  Step & \multicolumn{8}{l|}{External Event Subsequence} & \textproc{TEST} \\
\hline
1 & $e_1$   & $e_2$   & $e_3$   & $e_4$   & $\cdot$ & $\cdot$ & $\cdot$ & $\cdot$ & $\PASS$ \\
2 & $\cdot$ & $\cdot$ & $\cdot$ & $\cdot$ & $e_5$   & $e_6$   & $e_7$   & $e_8$   & $\PASS$ \\
3 & $e_1$   & $e_2$   & $\cdot$ & $\cdot$ & $e_5$   & $e_6$   & $e_7$   & $e_8$   & $\PASS$ \\
4 & $\cdot$ & $\cdot$ & $e_3$   & $e_4$   & $e_5$   & $e_6$   & $e_7$   & $e_8$   & $\DFAIL$ \\
5 & $\cdot$ & $\cdot$ & $e_3$   & $\cdot$ & $e_5$   & $e_6$   & $e_7$   & $e_8$   & $\DFAIL\ $($e_3$ found) \\
6 & $e_1$   & $e_2$   & $e_3$   & $e_4$   & $e_5$   & $e_6$   & $\cdot$ & $\cdot$ & $\DFAIL$ \\
7 & $e_1$   & $e_2$   & $e_3$   & $e_4$   & $e_5$   & $\cdot$ & $\cdot$ & $\cdot$ & $\PASS\ $($e_6$ found) \\
\hline
Result & $\cdot$   & $\cdot$   & $e_3$   & $\cdot$   & $\cdot$   & $e_6$ & $\cdot$ & $\cdot$ &
\end{tabular}
\caption{\label{fig:ddmin_example} Example execution of Delta Debugging,
taken from~\cite{Zeller:1999:YMP:318773.318946}.
`$\cdot$' denotes an excluded original external event.}
\end{table}

% \begin{figure}[tb]
%     \includegraphics[width=3.25in]{ddmin_execution.png}
%     %\vspace{-2em}
%     \caption{\label{fig:ddmin_example} Example execution of Delta Debugging.
%     The root cause}
%     \vspace{-1em}
% \end{figure}

\section{DPOR}
\label{app:dpor}

We show the original depth-first version of Dynamic Partial Order Reduction
in Algorithm~\ref{alg:dpor}. Our modified DPOR algorithm uses a priority queue
rather than a (recursive) stack, and tracks which schedules it has explored
in the past. Tracking which schedules we have explored in the past is
necessary to avoid exploring redundant schedules (an artifact of our non depth-first
exploration order). The memory footprint required for tracking previously explored
schedules continues growing for every new schedule we explore. Because we
assume a fixed time budget though,
we typically exhaust our time budget before \sys~runs out of memory.

There are a few desirable properties of DPOR we want to maintain,
despite our prioritized exploration order:

\noindent{\textbf{Soundness:}} any executed schedule should be valid, i.e. possible
to execute on an uninstrumented version of the program starting from the
initial configuration.

\noindent{\textbf{Efficiency:}} the happens-before partial order for every executed schedule
should never be a prefix of any other partial orders that have been
previously explored.

\noindent{\textbf{Completeness:}} when the state space is acyclic, the strategy is guaranteed to
find every possible safety violation.

Because we experimentally execute each schedule, soundness is easy to
ensure (we simply ensure that we do not violate TCP semantics if the application
assumes TCP, and we make sure that
we cancel timers whenever the application asks to do so).
Improved efficiency is the main contribution of partial order reduction. The last
property---completeness---holds for our modified version of DPOR so long as we
always set at least as many backtrack points as depth-first
DPOR.

\begin{algorithm*}[tb!]
\begin{algorithmic}
\State Initially: \Call{Explore}{$\emptyset$}
\Procedure{Explore}{$S$}
\State{$\kappa \gets last(S)$}
\ForAll{message $m \in pending(\kappa)$}
\If{$\exists i = max(\{i \in dom(S) | S_i$ is dependent and may be coenabled
with $next(\kappa,m)$ and $i \not\rightarrow_S m \})$}
\State{$E \gets \{m' \in enabled(pre(S,i)) | m' = m$ or $\exists j \in dom(S)
: j > i$ and $m' = msg(S_j)$ and $j \rightarrow_S m \}$}
\If{$E \neq \emptyset$}
\State{add any $m' \in E$ to $backtrack(pre(S,i))$}
\Else
\State{add all $m \in enabled(pre(S,i))$ to $backtrack(pre(S,i))$}
\EndIf
\EndIf
\EndFor

\If{$\exists m \in enabled(\kappa)$}
\State{$backtrack(\kappa) \gets \{m\}$}
\State{$done \gets \emptyset$}
\While{$\exists m \in (backtrack(\kappa) \setminus done)$}
\State{add $m$ to $done$}
\State{\Call{Explore}{$S.next(\kappa,m)$}}
\EndWhile
\EndIf

\EndProcedure
\end{algorithmic}
\caption{{\label{alg:dpor} The original depth-first version of Dynamic Partial Order Reduction
from~\cite{flanagan2005dynamic}. $last(S)$ denotes the configuration reached
after executing $S$;
$next(\kappa,m)$ denotes the state transition (message delivery) where the message m is
processed in configuration $\kappa$; $\rightarrow_S$ denotes `happens-before';
$pre(S,i)$ refers to the configuration where the transition $t_i$ is executed; $dom(S)$ means the set
$\{1,\dots,n\}$; $S.t$ denotes
$S$ extended with an additional transition $t$.}}
\end{algorithm*}

\section{Programmer Effort}
\label{app:programmer_effort}

In Table~\ref{tab:programmer_tasks} we summarize the various tasks, both
optional and necessary, that we assume programmers
complete in order to test and minimize using~\sys.

\begin{table}[tb]
\centering
\footnotesize
\begin{tabular}{l|l}
\textbf{Programmer-provided Specification} & \textbf{Default} \\
\hline
Initial cluster configuration & - \\
External event probabilities & No external events \\
%Scheduling discipline & UDP \\
Invariants & Uncaught exceptions \\
Violation fingerprint & Match on any violation \\
Message fingerprint function & Match on message type \\
Non-determinism mitigation & Replay multiple times
\end{tabular}
\caption{\label{tab:programmer_tasks} Tasks we assume the application
programmer completes in order to test and minimize using~\sys. Defaults
of `-' imply that the task is not optional.}
\end{table}


\section{Raft Case Studies}
\label{app:raft_case_studies}

Raft is a consensus protocol, designed to replicate a fault tolerant linearizable log of
client operations. akka-raft is an open source implementation of Raft.

The external events we inject for akka-raft case studies are
bootstrap messages (which
processes use for discovery of cluster members)
and client transaction requests. Crash-stop failures are indirectly triggered through
fuzz schedules that emulate network partitions.
The cluster size was 4 nodes
(quorum size=3) for all akka-raft case studies.
% akka-raft does not yet support crash-recovery.

The invariants we checked for akka-raft are the consensus invariants specified in
Figure 3 of the Raft paper~\cite{ongaro2014search}: Election Safety
(at most one leader can be elected in a
given term), Log Matching (if two logs contain an entry with the same
index and term, then the logs are identical in all entries
up through the given index), Leader Completeness (if a log entry is
committed in a
given term, then that entry will be present in the logs
of the leaders for all higher-numbered terms), and State Machine Safety
(if a server has applied a log entry
at a given index to its state machine, no other server
will ever apply a different log entry for the same index). Note that a
violation of any of these invariants allows for the possibility for the system
to later violate
the main linearizability invariant (State Machine Safety).

For each of the bugs where we did not initially know the root cause, we
started debugging by first minimizing the failing execution. Then,
we walked through the sequence of message deliveries in the
minimized execution. At each step,
we noted the current state of the actor receiving the message. Based on
our knowledge of the way Raft is supposed to work, we found places in
the execution that deviate from our understanding of correct behavior.
We then examined the code to understand why it deviated, and came up
with a fix. Finally, we replayed to verify the bug fix.

The akka-raft case studies in Table~\ref{dtab:case_studies} are shown in the
order that we found or reproduced them. To prevent bug causes from interfering
with
each other, we fixed all other known bugs for each case study. We reported all bugs and fixes to the akka-raft
developers.

\noindent{\textbf{raft-45: Candidates accept duplicate votes from the same
election term.}} Raft is specified as a state machine with three
 states: Follower, Candidate, and Leader. Candidates attempt to
 get themselves elected as leader by soliciting a quorum of
 votes from their peers in a given election term (epoch).

In one of our early fuzz runs, we found a violation of `Leader Safety',
 i.e. two processes believed they were leader in the same election term.
 This is a highly problematic situation for Raft to be in, since the leaders may overwrite each
 others' log entries, thereby violating the key linearizability guarantee that
 Raft is supposed to provide.

The root cause for this bug was that akka-raft's candidate
 state did not detect duplicate votes from the same follower
 in the same election term. (A follower might resend votes because it
 believed that an earlier vote was dropped by the network).
 Upon receiving the duplicate vote, the candidate counts it as a new vote and steps
 up to leader before it actually achieved a quorum of votes.

\noindent{\textbf{raft-46: Processes neglect to ignore certain
 votes from previous terms.}} After fixing the previous bug, we found another
 execution where two leaders were elected in the same term.

In Raft, processes attach an `election term' number to all messages they send.
 Receiving processes are supposed to ignore any messages that contain an election
 term that is lower than what they believe is the current term.

akka-raft properly ignored lagging term numbers for some, but not all
 message types. \sys~delayed the delivery of messages from previous terms and
 uncovered a case where a candidate incorrectly accepted a vote message from a previous election term.

\noindent{\textbf{raft-56: Nodes forget who they voted for.}} akka-raft is written as a finite state machine.
When making a state transition, FSM processes specify both which state they want to transition to, and which instance variables they want to keep once they have transitioned.

All of the state transitions for akka-raft were correct except
one: when the Candidate
steps down to Follower (e.g., because it receives an `AppendEntries' message,
indicating that there is another leader in the cluster), it
{\em forgets} which node it previously voted for in that term. Now, if another
node requests a vote from it in the same term, it may vote for
a different node than it previously voted for in the same term, later causing
two leaders to be elected, i.e. a violation of Raft's ``Leader Safety''
condition. We discovered this by manually examining the state
transitions made by each process throughout the minimized execution.

\noindent{\textbf{raft-58a: Pending client commands delivered
 before initialization occurs.}} After ironing out leader election issues, we started finding other issues.
 In one of our fuzz runs, we found that a leader process threw an assertion error.

When an akka-raft Candidate first makes the state transition to leader, it does not immediately initialize its
 state (the `nextIndex' and `matchIndex' variables). It instead sends a message to itself, and initializes its state when it receives that self-message.

Through fuzz testing, we found that it is possible that the Candidate could have pending
 ClientCommand messages in its mailbox, placed there before the Candidate transitioned to Leader
 and sent itself the initialization message. Once in the Leader state, the Akka runtime will first deliver
 the ClientCommand message. Upon processing the ClientCommand message the Leader tries to replicate it to
 the rest of the cluster, and updates its nextIndex hashmap. Next, when the Akka runtime delivers the
 initialization self-message, it will overwrite the value of nextIndex. When it reads from nextIndex later,
 it is possible for it to throw an assertion error because the nextIndex values are inconsistent with
 the contents of the Leader's log.


\noindent{\textbf{raft-58b: Ambiguous log indexing.}} In one of our fuzz tests,
 we found a case where the `Log Matching' invariant was violated, i.e. log entries did not appear in the same order on all machines.

According to the Raft paper, followers should reject AppendEntries requests from leaders that are behind,
 i.e. prevLogIndex and prevLogTerm for the AppendEntries message are behind what the follower has in its log.
 The leader should continue decrementing its nextIndex hashmap until the followers stop rejecting its AppendEntries attempts.

This should have happened in akka-raft too, except for one hiccup: akka-raft decided to adopt 0-indexed logs,
 rather than 1-indexed logs as the paper suggests. This creates a problem: the initial value of prevLogIndex is ambiguous:
Followers can not distinguish between an AppendEntries for an empty log (prevLogIndex == 0)
an AppendEntries for the leader's 1st command (prevLogIndex == 0), and
an AppendEntries for the leader's 2nd command (prevLogIndex == 1 â€“ 1 == 0).
The last two cases need to be distinguishable. Otherwise followers will not be able to reject inconsistent logs. This corner would have been hard to anticipate; at first glance it seems fine to adopt the convention that logs should be 0-indexed instead of 1-indexed.

As a result of this ambiguity, followers were unable to correctly reject AppendEntries requests from leader that were behind.

\noindent{\textbf{raft-42: Quorum computed incorrectly.}} We also found a fuzz test that
 ended in a violation of the `Leader Completeness' invariant, i.e. a newly elected leader
 had a log that was irrecoverably inconsistent with the logs of previous leaders.

Leaders are supposed to commit log entries to their state machine when they knows that
 a quorum (N/2+1) of the processes in the cluster have that entry replicated in their logs.
 akka-raft had a bug where it computed the highest replicated log index incorrectly.
 First it sorted the values of matchIndex (which denote the highest log entry index known
 to be replicated on each peer). But rather than computing the median (or more specifically,
 the N/2+1'st) of the sorted entries, it computed the mode of the sorted entries.
 This caused the leader to commit entries too early, before a quorum actually had that entry
 replicated. In our fuzz test, message delays allowed another leader to become elected, but
 it did not have all committed entries in its log due to the previously leader committing too soon.

As we walked through the minimized execution, it
became clear mid-way through the execution that not all
entries were fully replicated when the master committed
its first entry. Another process without all replicated entries
then became leader, which constituted a violation
of the ``Leader Completeness'' invariant.

\noindent{\textbf{raft-66: Followers unnecessarily overwrite log entries.}} The last issue we found is only possible to
 trigger if the underlying transport protocol is UDP, since it requires reorderings of messages between the same source, destination pair.
 The akka-raft developers say they do not currently support UDP, but they
 would like to adopt UDP in the future due to its lower latency.

The invariant violation here was a violation of the `Leader Completeness' safety property,
 where a leader is elected that does not have all of the needed log entries.

Leaders replicate uncommitted ClientCommands to the rest of the cluster in batches. Suppose a follower with an empty log receives an AppendEntries containing two entries. The follower appends these to its log.

Then the follower subsequently receives an AppendEntries containing only the first of the previous two entries (this message was delayed). The follower will inadvertently delete the second entry from its log.

This is not just a performance issue: after receiving an ACK from the follower, the leader is under the
 impression that the follower has two entries in its log.
 The leader may have decided to commit both entries if a quorum was achieved.
 If another leader becomes elected, it will not necessarily have both
 committed entries in its log as it should, leading to a `LeaderCompleteness'
 violation.


\section{Spark Case Studies}
\label{app:spark_case_studies}

Spark is a large scale data analytics framework. We focused our efforts on
reproducing known bugs in the core Spark engine, which is responsible for
orchestrating computation across multiple machines.

We looked at the entire history of bugs reported for Spark's core engine. We found that most reported bugs only involve sequential
computation on a single machine (e.g. crashes due to unexpected user input).
We instead focused on reported bugs involving concurrency across machines or
partial failures. Of the several dozen reported concurrency or partial failure
bugs, we chose three.

The external events we inject for Spark case studies are worker join events
(where worker nodes join the cluster and register themselves with the master),
job submissions, and crash-recoveries of the master node. The Spark job we ran
for all case studies was a simple parallel approximation of the digits of Pi.


%  - Note on Spark's message complexity:
%      https://docs.google.com/document/d/1xlDmaACBTCkDioydjUnpQzr6j-2NQPl-3Zj_w23stSg/edit

% ------------------- spark-2294 -------------------
\noindent{\textbf{spark-2294: Locality inversion.}} In Spark, an `executor' is
responsible for performing computation for Spark jobs. Spark jobs are assigned
`locality' preferences: the Spark scheduler is supposed to launch
`NODE\_LOCAL' tasks (where the input data for the task is located on the same
machine) before launching tasks without preferences. Tasks without locality
preferences are in turn
supposed to be launched before `speculative' tasks.

The bug for this case
study was the following: if an executor E is free, a task may be speculatively assigned to E when there are other tasks in the job
that have not been launched (at all) yet. Similarly, a task without any locality preferences may be
assigned to E when there was another `NODE\_LOCAL' task that could have been scheduled.
The root cause of this bug was an error in Spark scheduler's logic: under
certain configurations of pending Spark jobs and currently available
executors, the Spark scheduler would incorrectly invert the locality priorities.
We reproduced this bug by injecting random, concurrently running Spark jobs
(with differing locality preferences) and random worker join events.

% This happens because Spark's scheduler (TaskSchedulerImpl) calls a priority function with increasing locality levels, beginning with PROCESS_LOCAL,
% followed by NODE_LOCAL, and so on until the highest currently allowed level.
% Now, supposed NODE_LOCAL is the highest currently allowed locality level.
% The first time findTask is called, it will be called with max level PROCESS_LOCAL;
% if it cannot find any PROCESS_LOCAL tasks, it will try to schedule tasks with no locality preferences
% or speculative tasks. As a result, speculative tasks or tasks with no preferences may be scheduled
% instead of NODE_LOCAL tasks.

% ------------------- spark-3150 -------------------
\noindent{\textbf{spark-3150: Simultaneous failure causes infinite restart
loop.}} Spark's
master node supports a `Cold-Replication' mode, where it commits its state to
a database (e.g., ZooKeeper). Whenever the master node crashes, the node that
replaces it can read that information from the database to bootstrap its
knowledge of the cluster state.

To trigger this bug, the master node and the driver process need to fail
simultaneously. When the master node restarts, it tries to read its state from
the database. When the driver crashes simultaneously, the
information the master reads from the database is corrupted: some of
the pointers referencing information about the driver are null. When the
master reads this information, it dereferences a null pointer and crashes
again. After failing, the master restarts, tries to recover its state, and
crashes in an infinite cycle. The minimized execution for this bug contained
exactly these 3 external events, which made the problematic code path immediately apparent.

\noindent{\textbf{spark-9256: Delayed message causes master crash.}} We found the
following bug through fuzz testing.

As part of
initialization, Spark's client driver registers with the Master node by repeatedly sending a RegisterApplication
  message until it receives a RegisteredApplication response.
 If the RegisteredApplication response is delayed by at least as long as the
 configured timeout value (or if the network
  duplicates the RegisterApplication RPC), it is possible for the Master to receive two RegisterApplication
  messages for the same client driver.

Upon receiving the second RegisterApplication message, the master attempts to
persist information about the client driver
to disk. Since the file containing information about the client driver already exists though, the master crashes with an
IllegalStateException.

This bug is possible to trigger in production, but it will occur only very
rarely. The name of the file containing information has a second-granularity timestamp associated
with it, so it would only be possible to have a duplicate file if the second
RegisteredApplication response arrived in the same second as the first
response.

% \section*{Acknowledgements}
% 
% We thank our shepherd David Lie and the anonymous reviewers for their feedback.
% We also thank Peter Alvaro, Barath Raghavan, and Kay Ousterhout for feedback
% on the submitted draft. This research was supported by NSF CNS 1040838 and a
% gift from Intel. Colin Scott was also supported by an NSF Graduate Research
% Fellowship.

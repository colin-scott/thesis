%\colin{TODO: emphasize blackbox vs whitebox minimization?}
%\colin{TODO: Admit up front that, most bugs, even in distributed systems, can
%be minimized with traditional delta debugging (e.g. unexpected configurations,
%unexpected messages, etc.). However, the bugs that are the most difficult are
%those that involve concurrency \& asynchrony.}

Discuss two challenges: intractability, and non-determinism. In ~\ref{}... we
address. In ~\ref{}... we address.

We
focus on executions generated by fuzz testing, but we also
illustrate how one might reduce production traces.

We address this
challenge by
instrumenting the Akka actor system framework~\cite{akka} to gain nearly perfect control over when
events occur.


In this chapter, we seek to generalize our investigation from
chapter~\ref{sec:sts} along several dimensions.
The \projectname~system described in chapter~\ref{sec:sts} targeted a specific distributed system (SDN controllers), and focused
on reducing input events given limited control over the execution.
Here we target a broader range of systems, define the
general problem of execution reduction, exercise significantly greater
control, and systematically explore the state space. We also articulate new
reduction
strategies that quickly reduce input events,
internal events, and message contents.

% TODO(cs): highlight a key question

%In our evaluation, we provide
% empirical comparisons of old and new scheduling strategies.

% TODO(cs): what can DEMi not do?

Our tool, \system~(\sys), is implemented in {\footnotesize $\sim$}14,000
lines of Scala. We have applied \sys~to akka-raft~\cite{akka-raft}, an open
source Raft consensus implementation, and Apache Spark~\cite{zaharia2012resilient}, a widely used data analytics framework.
Across 10 known and discovered bugs, \sys~produces executions that are within
a factor of 1X to 4.6X (1.6X median) the size of the smallest possible
bug-triggering execution,
and between 1X and 16X (4X median) smaller than the
executions produced by the previous state-of-the-art blackbox technique
(\S\ref{sec:sts}).
The results we find for these two very different systems leave us optimistic that these
techniques, along with adequate visibility into events (either through a
framework like Akka, or through
custom monitoring), can be applied successfully to a wider range of systems.

%\colin{"Integrated programming environment"}
%\colin{articulate tradeoff between overly-specific vs overly-general
% minimization specifications. Have this as part of a discussion section?}

% Contributions:
%   https://docs.google.com/document/d/1Y5WBlv6iul-HP74DM37gTfx1tzUtfZTowqD4Db11US4/edit
% Limitations of the SIGCOMM paper:
%
% limited control over the execution
%   execution is not linearizable -> depend on wall-clock time to infer whether expected events are or are not going to show up during replay
%   limited to no control over sources of non-determinism
% no clean formulation
%   no intuition for why the heuristics work
% narrow system target (SDN controllers)
%
% Contributions of the NSDI paper:
%
% broader system target
%   - Spark, Raft, JavaScript, reliable broadcast, {SDN?, DHT?, distributed database?}
%   - multiple bug types for each
% more control over the execution
%   - actor model gives us atomicity (linearizability): we know exactly when the actor has fully processed the last message we delivered to it -> no need to wait on wall-clock to see whether it’s going to send some expected message
%   - narrower API also helps us mitigate non-determinism somewhat
% clean problem formulation
%   - lay out all degrees of freedom: removal of external events, network schedule, external message contents, cluster size, etc.
%   - articulate what would be required to arrive at the ideal output (build an intuition for possible heuristics) & explain why that’s intractable. Instead: we have a fixed computational budget, and we want to maximize minimization within the budget.
%   - reordering of messages (edit distance)
%   - minimization of internal messages, in addition to external messages
%   - shrinking of message contents, cluster size
%   - module minimization?
% empirical data on effectiveness of heuristics
%   - compare minimality, runtime complexity for each of the heuristics. Specifically show how much each heuristic buys beyond the SIGCOMM heuristics.
%   - for a sampling of bugs, also compare against ground truth: either (i) a case where know by inspection exactly what the MCS should be, or (ii) a case where we are able to run DPOR to completion.
% conjectures (claims?) on why heuristics are effective
%   - explanation of the nature of QA tests, in terms of what code paths they tend to exercise
%   - computational models: constraints on how programs behave in practice. Examples: data-independence (fingerprinting), commutativity (DPOR), recency of state, bounded-edit-distance
%
%
% Delta from related work (also see: STS FAQ):
%
% traditional minimization (DDmin, QuickCheck’s shrinking)
%   not immediately applicable to systems with intermediate external events
% best-effort minimization (QuickCheck applied to concurrent systems, field failures) 
%   don’t systematically handle concurrency
% model checking minimization: MAX-Sat, interpolation (ConcBugAssist, DSPs)
%   many disadvantages of model checking. See STS FAQ.
% deterministic replay log minimization (schedule minimization, program analysis)
%   don’t allow divergence during replay! i.e. minimization specification is overly specific -> minimization results aren’t great
%   program flow analysis work is tied to a single language, + high perf overhead
% model inference, log summarization (synoptic, csight, invariant mining)
%   summarize the events that occurred, but don’t actually minimize the test case. Apply this after you minimize the test case with our techniques.
% program slicing, automated debugging (experimental, program analysis)
%   minimize program statements, but not the test case itself. Useful for debugging, but not troubleshooting. Apply this after you minimize the test case with our techniques.
%
% STS FAQ:
%
% why not analyze the program?
%   ties you to one language
%   parsers usually aren’t complete; will sometimes have to make assumptions that aren’t realistic
%   requires engineering effort to get working
%   high computational overhead of tracing
%   by solving a harder problem, we come up with more interesting solutions
%
% Is QA testing going to be around forever? Why not use newer more sophisticated testing tools?
%   As long as you have bugs in production, you will need minimization.
%
% Why is previous work (STS) unsatisfying?
%   we know there are some MCSes that are not minimal
%   no empirical comparison of heuristics vs. other heuristics
%   no intuition for why heuristics work, or how to come up with other heuristics
%
% Why is minimization needed?
%
% How do we deal with non-determinism?
%
% Why be loosey-goosey about which specific safety violation we trigger? (why not require exact same failing code path?)

% TODO(cs): what kinds of distributed systems are we targeting? Primarily
% control plane systems, which are designed to operate indefinitely.
% TODO(cs): in the introduction, illustrate a compelling example that shows
% why 1000s of events are a difficult starting point for debugging.

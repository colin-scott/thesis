%\setlength{\textfloatsep}{2.0pt plus 2.0pt minus 4.0pt} % Reset \textfloatsep

% TODO(cs): discuss Akka's use of TCP vs UDP?

\begin{table*}
{
\centering
\footnotesize
\begin{tabular}{|l|l|l|l|l|l||l||l|}
  \cline{1-8}
  \textbf{Bug Name} & \textbf{Bug Type} &
  \textbf{Initial} & \textbf{Provenance} &
  \textbf{STSSched} & \textbf{TFB} & \textbf{Optimal} & \textbf{NoDiverge} \\\cline{1-7} \hline
\href{https://docs.google.com/document/d/1alldH4lRSpFQ55-YCNFaIcMpjxrBOMNaB7R_9eEVwfs}{raft-45}
& Akka-FIFO, reproduced
& 2160\hfill (E:108) & 2138\hfill (E:108) & 1183\hfill (E:8) & 23 \hfill (E:8)
& 22 \hfill (E:8) & 1826 \hfill (E:11) \\

% 3 node opt
\href{https://docs.google.com/document/d/1vhGftMIlmm_uFLmCz2GI0CCPmttDUR617hJ72k2BTbY}{raft-46}
& Akka-FIFO, reproduced
& 1250\hfill (E:108) & 1243 \hfill (E:108) & 674\hfill (E:8) & 35 \hfill (E:8)
& 23 \hfill (E:6) & 896 \hfill (E:9) \\

\href{https://docs.google.com/document/d/1_UPKhjYoSrG9p4FQXqML_WQKa3WNbM2fch2aa0V-9Go}{raft-56}
& Akka-FIFO, found
& 2380 \hfill (E:108) & 2376 \hfill (E:108) & 1427 \hfill (E:8) & 82 \hfill
(E:8) & 21 \hfill (E:8) & 2064 \hfill (E:9) \\

\href{https://docs.google.com/document/d/1_efq5rbOGCG3sG-2qxbbOeIRztW9XYP6xgyh0QFmWXg}{raft-58a}
% raft-58-initialization
& Akka-FIFO, found
& 2850 \hfill (E:108) & 2824 \hfill (E:108) & 953 \hfill (E:32) & 226 \hfill
(E:31) & 51 \hfill (E:11) & 2368 \hfill (E:35) \\

\href{https://docs.google.com/document/d/1eERBdohTC3UidHJ6cZq12ixpp_eiVqzT7LvzuTeEtR4}{raft-58b}
% raft-58-log-match
& Akka-FIFO, found
& 1500\hfill (E:208) & 1496\hfill (E:208) & 164\hfill (E:13) & 40\hfill (E:8)
& 28 \hfill (E:8) & 1103 \hfill (E:13) \\

\href{https://docs.google.com/document/d/1BN4hpTKtN_-inqyT5XTGogI8QSlohnP2Y7A01nuW7TI}{raft-42}
& Akka-FIFO, reproduced
& 1710 \hfill (E:208) & 1695 \hfill (E:208) & 1093 \hfill (E:39) & 180 \hfill
(E:21) & 39 \hfill (E:16) & 1264 \hfill (E:43) \\

\href{https://docs.google.com/document/d/1lCAR_IE_U27CA-VTlPCi4Kyxf4h6Ykx-0zbkHe0Gfbg}{raft-66}
& Akka-UDP, found
& 400\hfill (E:68) & 392\hfill (E:68) & 262 \hfill (E:23) & 77 \hfill (E:15) &
29 \hfill (E:10) & 279 \hfill (E:23) \\

\href{https://docs.google.com/document/d/1JQiaLlm6XwLCWKOBOFkaMzg8Gd7NT5gDH3_8OV-oekY}{spark-2294}
& Akka-FIFO, reproduced
& 1000 \hfill (E:30) & 886 \hfill (E:30) & 43 \hfill (E:3) & 40 \hfill (E:3) &
25 \hfill (E:1) & 43 \hfill (E:3) \\

% \href{https://docs.google.com/document/d/1g4jT83N6proW6YwHHhWsFy3hQ-ym2EaeDl24scDY6mY}{spark-2294-caching}
% & Akka-FIFO, reproduced
% & 700 \hfill (E:3) & 667 \hfill (E:3) & 64 \hfill (E:3) & 51 \hfill (E:3) & -
% - \hfill (E:1) \\
% % TODO(cs): only 3 initial external events?

\href{https://docs.google.com/document/d/1mWbxS2-B0v4fbY_FIASs6HCmAXFh6K89DMYX-XEmeL8}{spark-3150}
& Akka-FIFO, reproduced
& 600 \hfill (E:20) & 536 \hfill (E:20) & 18 \hfill (E:3) & 14 \hfill (E:3) &
11 \hfill (E:3) & 18 \hfill (E:3) \\

\href{https://docs.google.com/document/d/1rPseAbo3FELT8Fj2lrRUMkrDmjLPYI26Jgr8goAMLzU}{spark-9256}
& Akka-FIFO, found (rare)
& 300 \hfill (E:20) & 256 \hfill (E:20) & 11 \hfill (E:1) & 11 \hfill (E:1) &
11 \hfill (E:1) & 11 \hfill (E:1) \\

\hline
\end{tabular}
\caption{Overview of case studies. ``E:'' is short for ``Externals:''. The
`Provenance', `STSSched', and `TFB' techniques are pipelined one after another.
`Initial' minus `TFB' shows overall reduction; `Provenance' shows how many events can be statically
removed; `STSSched' minus `TFB' shows how our new techniques
compare to the previous state of the art~\cite{sts2014};
 `TFB' minus `Optimal' shows how far from optimal our results are; and
`NoDiverge' shows the size of minimized executions when no divergent schedules are explored (explained
in \S\ref{sec:related_work}).}
\label{dtab:case_studies}
%\vskip -2em
}
\end{table*}

\begin{table}
\centering
\footnotesize
\begin{tabular}{|l|l|l|}
  \cline{1-3}
  \textbf{Bug Name} &
  \textbf{STSSched} & \textbf{TFB} \\\cline{1-3} \hline
\href{https://docs.google.com/document/d/1alldH4lRSpFQ55-YCNFaIcMpjxrBOMNaB7R_9eEVwfs}{raft-45}
& 56s \hfill (594) & 114s \hfill (2854) \\
% Total: 170s
\href{https://docs.google.com/document/d/1vhGftMIlmm_uFLmCz2GI0CCPmttDUR617hJ72k2BTbY}{raft-46}
& 73s \hfill (384) & 209s \hfill (4518) \\
% Total: 282s
\href{https://docs.google.com/document/d/1_UPKhjYoSrG9p4FQXqML_WQKa3WNbM2fch2aa0V-9Go}{raft-56}
& 54s \hfill (524) & 2078s \hfill (31149) \\
% Total: 2132s
\href{https://docs.google.com/document/d/1_efq5rbOGCG3sG-2qxbbOeIRztW9XYP6xgyh0QFmWXg}{raft-58a}
& 137s \hfill (624) & 43345s \hfill (834972) \\
% Total: 43482s
\href{https://docs.google.com/document/d/1eERBdohTC3UidHJ6cZq12ixpp_eiVqzT7LvzuTeEtR4}{raft-58b}
& 23s \hfill (340) & 31s \hfill (1747) \\ % Seems suspect..
% Total: 69s
\href{https://docs.google.com/document/d/1BN4hpTKtN_-inqyT5XTGogI8QSlohnP2Y7A01nuW7TI}{raft-42}
& 118s \hfill (568) & 10558s \hfill (176517) \\
% Total: 10676s
\href{https://docs.google.com/document/d/1lCAR_IE_U27CA-VTlPCi4Kyxf4h6Ykx-0zbkHe0Gfbg}{raft-66}
& 14s \hfill (192) & 334s \hfill (10334) \\ % Seems suspect..
% Total: 348s
\href{https://docs.google.com/document/d/1JQiaLlm6XwLCWKOBOFkaMzg8Gd7NT5gDH3_8OV-oekY}{spark-2294}
& 330s \hfill (248) & 97s \hfill (78)  \\
% Total: 427s

% \href{https://docs.google.com/document/d/1g4jT83N6proW6YwHHhWsFy3hQ-ym2EaeDl24scDY6mY}{spark-2294-caching}
% & 828s \hfill (467) & 270s \hfill (179) \\
% % Total: 1098s

\href{https://docs.google.com/document/d/1mWbxS2-B0v4fbY_FIASs6HCmAXFh6K89DMYX-XEmeL8}{spark-3150}
& 219s \hfill (174) & 26s \hfill (21) \\ % Suspect?
% Total: 245s
\href{https://docs.google.com/document/d/1rPseAbo3FELT8Fj2lrRUMkrDmjLPYI26Jgr8goAMLzU}{spark-9256}
& 96s \hfill (73) & 0s \hfill (0) \\
% Total: 210s
\hline
\end{tabular}
%\vskip -1em
\caption{Minimization runtime in seconds (total schedules executed). Overall runtime is
the summation of ``STSSched'' and ``TFB''. spark-9256 only
had unignorable events remaining after STSSched completed, so TFB was not
necessary.}
\label{tab:runtime}
%\vskip -1em
\end{table}

% TODO(cs): auxiliary metric for MCSes: not the size of the MCS, but instead
% its 'depth', i.e. how many parallel stages are contained in it. More
% generally, could consider applying Sylvia's NetComplex metric to our
% executions.
% TODO(cs): compare against "without-interposition", i.e. where we don't muck
% with the akka runtime.

Our evaluation focuses on two key metrics: (i) the size of
the reproducing sequence found by \sys, and (ii) how quickly
\sys~is able to make minimization progress within a fixed time
budget. We show a high-level overview
of our results in Table~\ref{dtab:case_studies}.
The ``Bug Type'' column shows two pieces of information: whether the bug can be
triggered using TCP semantics (denoted as ``FIFO'')
% assuming a source, destination FIFO scheduling discipline (i.e.
% assuming TCP)
or whether it can only be triggered when UDP is used; and whether we discovered
the bug ourselves or whether we
reproduced a known bug. The ``Provenance'' column shows how many events from the initial execution
remain after statically pruning events
that are concurrent with the safety violation.
The ``STSSched'' column shows how many events remain after checking the initial schedules
prescribed by our prior work~\cite{sts2014} for each of delta debugging's
subsequences. The ``TFB'' column shows the final execution size after
we apply our techniques (`Type Fingerprints with Backtracks'), where we direct
DPOR to explore as many backtrack points that match the types of original
messages (but no other
backtrack points) as possible within the 12 hour time budget we provided.
Finally, the ``Optimal'' column shows the size of the smallest
violation-producing execution we could construct by hand.
We ran all experiments on a 2.8GHz Westmere processor with 16GB memory.

Overall we find
that \sys~produces executions that are within
a factor of 1X to 4.6X (1.6X median) the size of the smallest possible execution
that triggers that bug, and between 1X and 16X (4X median) smaller than the
executions produced by our previous technique (STSSched).
STSSched is effective at minimizing external events (our primary minimization
target) for most case studies. TFB is
significantly more effective for minimizing internal events (our secondary
target), especially for akka-raft.
Replayable executions for all case studies are available at
\href{https://github.com/NetSys/demi-experiments}{github.com/NetSys/demi-experiments}.

% TODO(cs): quote how many traces are close to optimal, vs. quoting trace
% reduction.

We create the initial executions for all of our case studies by generating fuzz tests with \sys~(injecting a fixed number of random external events, and selecting internal
messages to deliver in a random order)
and selecting the first execution that triggers the invariant
violation with $\geq$300 initial message deliveries.
Fuzz testing terminated after finding a faulty execution within 10s of minutes for most of our case studies.

For case studies where the bug was previously known,
we set up the initial test conditions (cluster configuration, external events) to
closely match those described in the bug report. For cases where we discovered
new bugs, we set up the test environment to explore situations that developers
would likely encounter in production systems.
% TODO(cs): mention that we plan to show a more robust evalution in the
% future: take N randomly generated, failing fuzz runs for each bug, see how
% well we do across all of them. e.g. show the median, mean, deviation of our
% minimization.

% TODO(cs): cut? if we need space.
As noted in the introduction, the systems we focus on are
akka-raft~\cite{akka-raft} and Apache Spark~\cite{zaharia2012resilient}.
akka-raft, as an early-stage software project, demonstrates how~\sys~can aid the development process.
Our evaluation of Spark demonstrates that \sys~can be applied to complex, large scale distributed systems.
%; and the cluster management portion of
%OpenDaylight~\cite{odl}, an open source software-defined networking controller developed
%by over a dozen networking companies
%, and a Pastry DHT implementation,
%, and a reliable broadcast implementation,

\noindent{\textbf{Reproducing Sequence Size.}} We compare the size of the
minimized executions produced by \sys~against the
smallest fault-inducing executions we could construct by hand (interactively
instructing \sys~which messages to deliver).
For 6 of our 10 case studies, \sys~was within a factor of 2 of optimal.
There is still room for improvement however. For raft-58a for example,
\sys~exhausted its time budget and produced an execution that was a
factor of 4.6 from optimal. It could have found a smaller execution without
exceeding its time budget with a better schedule exploration strategy.

%Quiescent reliable
%communication---that is, reliable communication that is guaranteed to
%eventually stop sending messages---can be implemented if a distributed system
%employs a failure detector~\cite{chandra1996unreliable,aguilera1997heartbeat}, but then the failure
%detector component itself never stops sending messages. Worse, even if a
%system is quiescent, we do not necessarily know {\em a priori} the exact
%point the system is going to stop messages. In short, comparing against
%an optimal search strategy is impossible for all but the simplest of
%distributed algorithms.

\eat{
The size of the output produced by
\sys~is shown in the \num{second to last} column of
Table~\ref{dtab:case_studies}, and the size
of our manually constructed traces is shown in the \num{final} column.\george{The last column does not seem to be the manual result}
}

\noindent{\textbf{Minimization Pace.}} To measure how quickly \sys~makes
progress, we graph schedule size as a function of the number of
executions~\sys tries.
Figure~\ref{fig:progress} shows an example for
raft-58b. The other case studies follow the same general pattern of sharply decreasing
marginal gains.

\begin{figure}[tb]
    \centering
    \includegraphics[width=3.25in]{demi/raft_58b_minimization_progress.pdf}
    %\vspace{-2em}
    \caption{\label{fig:progress} Minimization pace for raft-58b. Significant progress is made early on, then progress becomes rare.}
    %\vspace{-1em}
\end{figure}

% TODO(cs): I think raft-42 may also have exceeded its time budget, its just that the
% time-boxing is approximate, and the absolute time taken is sensitive to where DDmin
% makes progress.

We also show how much time (\# of replays) \sys~took to reach completion of
STSSched and TFB in Table~\ref{tab:runtime}.\footnote{It is important
to understand that \sys~is able to replay executions significantly more quickly
than the original execution may have taken. This is because~\sys~can trigger timer events before the wall-clock duration for
those timers has actually passed, without the application being aware of this
fact (cf.~\cite{Gupta06toinfinity})} The time budget we allotted to~\sys~for all case studies was 12 hours (43200s).
All case studies except raft-56, raft-58a, and raft-42 reached completion of TFB in less than 10
minutes.

\noindent{\textbf{Qualitative Metrics.}} We do not evaluate how minimization helps
with programmer productivity. Data on how humans do debugging is scarce; we
are aware of only one study that measures how
quickly developers debug minimized vs. non-minimized traces\cite{fse_web_ddmin}.
Nonetheless, since humans can only keep a small number of facts in working
memory~\cite{miller56seven}, minimization seems generally useful. As one
developer
puts it,
``Automatically shrinking test cases to the
minimal case is immensely helpful''\cite{riak_quote}.

% Personal email from Andreas Zeller:

% data on how humans do debugging and troubleshooting is pretty scarce.  It is
% not difficult to assume that humans will take less time to debug a program
% if the assumptions / executions / inputs are fewer / shorter / simpler; but
% how great the effect is, and by which properties it is most determined, is
% something we don’t know, in particular in the context of  program debugging.
% Good luck with your search!

% Cheers,
% Andreas


% TODO(cs): Show specifically that "fairness" is effective at distributing the
% time budget?

% It is possible to implement all of these systems in a way that
% uses failure detectors rather than timers. We either modify existing systems to do so, or we implemented the
% the system ourselves from the ground up to use only failure detectors.
% Crucially, we put the behavior of the failure detector under our control, so
% that it only sends out crash suspicions but not heartbeat messages.
%
% To incorporate failure detectors into
% our DPOR exploration, we begin by modeling a perfect failure detector (providing strong
% accuracy, i.e. no incorrect crash suspicions) as a first
% order approximation. Whenever DPOR injects a partition event, we have the
% failure detector immediately deliver {\tt NodeUnreachable(node)} messages to all other hosts involved
% in the partition.
%
% Note that there are two caveats to this approach. First, even a perfect
% failure detector is not required to immediately deliver failure notifications.
% It may deliver them any point after the failure, but before the end of the
% execution. In theory we would need to explore all valid placements of {\tt
% NodeUnreachable} deliveries. Second, it is tricky to incorporate network partitions into
% DPOR's notion of dependency analysis. See Vjeko's document for how we do this.
% It is worth noting that, ultimately, DPOR is an optimization; if we find that
% incorporating network partitions into DPOR is too difficult, we could just run
% na\"{\i}ve model checking.
%
% Later, we might try to model weaker failure detectors. We know that the weakest failure detector needed for consensus
% is the $\diamondsuit W$ failure detector~\cite{chandra1996weakest}, which
% guarantees that after some (unknown) finite time, there exists at least one
% non-crashed node which is never incorrectly suspected by all other nodes.
% Notice that $\diamondsuit W$ may incorrectly suspect nodes as crashed! To
% incorporate weaker failure detectors into our DPOR search, we would need to
% enumerate all possible placements of both correct crash suspicions and
% incorrect crash suspicions for all schedule of length $\le |\tau^{\circ}|$. It
% may be easiest to simply model $W$ rather than $\diamondsuit W$, since
% $\diamondsuit W$ requires that we also explore an infinite space of delays
% until the failure detector stabilizes.

\subsection{Raft Case Studies} Our first set of case studies are taken from
akka-raft~\cite{akka-raft}. akka-raft is implemented in 2,300 lines of Scala excluding tests. akka-raft has existing unit and integration tests, but it
has not been deployed in production. The known
bugs we reproduced had not yet been fixed; these were
found by a recent manual audit of the code.

For full descriptions of each case study, see \S\ref{app:raft_case_studies}.
The lessons we took away from our akka-raft case studies are twofold. First,
fuzz testing is quite effective for finding bugs in
early-stage software.
We found and fixed these
bugs in less than two weeks, and several of the
bugs would have
been difficult to anticipate a priori. Second, debugging unminimized faulty
executions would be very time consuming and conceptually challenging;
we found that the most fruitful debugging process was to walk through events
one-by-one to understand how the system arrived at the unsafe state, which would take hours for unminimized executions.

\subsection{Spark Case Studies}

Spark~\cite{spark_repo} is a mature software project, used
widely in production. The version of Spark we used for our evaluation consists of more
than 30,000 lines of Scala for just the core execution engine.
Spark is also interesting because it has a significantly different
communication pattern than
Raft (e.g., statically defined masters).

For a description of our Spark case studies, see \S\ref{app:spark_case_studies}.
Our main takeaway from Spark is that for the simple Spark jobs we submitted,
STSSched does
surprisingly well. We believe this
is because Spark's communication tasks were all almost entirely independent
of each other. If we had
submitted more complex Spark jobs with more dependencies between messages
(e.g. jobs that make use of intermediate
caching between stages) STSSched likely would not have performed as well.

% However, for more complex Spark jobs,
% particularly those that {\em cache}
% intermediate result to be used as inputs to later stages, STSSched does not minimize
% as effectively (as demonstrated in the spark-2294-caching case study, where we
% modified our Spark job to cache intermediate results).
% This is because the messages Spark sends to track the locations of cached
% intermediate results are {\em history-dependent}.

\subsection{Auxiliary Evaluation}

\noindent{\textbf{External message shrinking.}} We demonstrate the benefits of
external message shrinking with an akka-raft case study. Recall that akka-raft
processes receive an external bootstrapping message that informs
them of the IDs of all other processes. We started with a 9 node akka-raft
cluster, where we triggered the raft-45 bug.
We then shrank message contents by removing each element (process ID) of bootstrap messages,
replaying these along with all other events in the failing execution, and
checking whether the violation was still triggered. We were able to shrink the
bootstrap message contents from 9 process IDs to 5 process IDs. Finally, we
ran STSSched to completion, and compared the output to STSSched without the
initial message shrinking. The results shown in
Table~\ref{tab:message_shrinking} demonstrate that message shrinking can help
minimize both external events and message contents.

\begin{table}
\centering
\footnotesize
\begin{tabular}{|l|l|l|}
  \cline{2-3}
  \multicolumn{1}{c|}{~} & \textbf{Without Shrinking} & \textbf{With
  shrinking} \\\cline{2-3} \hline
{\bf Initial Events} & 360 \hfill (E: 9 bootstraps) & 360 \hfill (E: 9 bootstraps) \\
\hline
{\bf After STSSched} & 81 \hfill (E: 8 bootstraps) & 51 \hfill (E: 5 bootstraps) \\
\hline
\end{tabular}
%\vskip -1em
\caption{External message shrinking results for raft-45 starting with 9
processes. Message shrinking + minimization was able to reduce the cluster
size to 5 processes.}
\label{tab:message_shrinking}
%\vskip -1.5em
\end{table}

\noindent{\textbf{Instrumentation Overhead.}} Table~\ref{tab:instrumentation} shows
the complexity in terms of lines of Scala code needed to define message fingerprint
functions, mitigate non-determinism (with the application modifications
described in \S\ref{dsec:implementation}), specify invariants, and configure
\sys. In total we spent roughly one person-month debugging
non-determinism.
% For the non-determinism mitigation row, we exclude our AspectJ
% interposition on the Akka APIs.

\begin{table}
\centering
\footnotesize
\begin{tabular}{|l|l|l|}
  \cline{2-3}
  \multicolumn{1}{c|}{~} & \textbf{akka-raft} & \textbf{Spark} \\\cline{2-3} \hline
% 26 * 5 => atomic blocks
% 4 => sorting
% 4 => timeouts
% 6 + 7 => no exceptions
% 1 => no DNS
% 1 => use actor system
% 33 => actorBlocked
% 30 => run on a single JVM
{\bf Message Fingerprint} & 59 & 56 \\
\hline
{\bf Non-Determinism} & 2 & {\footnotesize $\sim$}250 \\
\hline
{\bf Invariants} & 331 & 151 \\
\hline
{\bf Test Configuration} & 328 & 445 \\
\hline
%{\bf Akka Interposition} & - & 336 \\
%\hline
\end{tabular}
%\vskip -1em
\caption{Complexity (lines of Scala code) needed to define message
fingerprints, mitigate non-determinism, define invariants, and configure \sys.
Akka API interposition (336 lines of AspectJ) is application independent.}
\label{tab:instrumentation}
%\vskip -1em
\end{table}

\subsection{Full Description of Raft Case Studies}
\label{app:raft_case_studies}

Raft is a consensus protocol, designed to replicate a fault tolerant linearizable log of
client operations. akka-raft is an open source implementation of Raft.

The external events we inject for akka-raft case studies are
bootstrap messages (which
processes use for discovery of cluster members)
and client transaction requests. Crash-stop failures are indirectly triggered through
fuzz schedules that emulate network partitions.
The cluster size was 4 nodes
(quorum size=3) for all akka-raft case studies.
% akka-raft does not yet support crash-recovery.

The invariants we checked for akka-raft are the consensus invariants specified in
Figure 3 of the Raft paper~\cite{ongaro2014search}: Election Safety
(at most one leader can be elected in a
given term), Log Matching (if two logs contain an entry with the same
index and term, then the logs are identical in all entries
up through the given index), Leader Completeness (if a log entry is
committed in a
given term, then that entry will be present in the logs
of the leaders for all higher-numbered terms), and State Machine Safety
(if a server has applied a log entry
at a given index to its state machine, no other server
will ever apply a different log entry for the same index). Note that a
violation of any of these invariants allows for the possibility for the system
to later violate
the main linearizability invariant (State Machine Safety).

For each of the bugs where we did not initially know the root cause, we
started debugging by first minimizing the failing execution. Then,
we walked through the sequence of message deliveries in the
minimized execution. At each step,
we noted the current state of the actor receiving the message. Based on
our knowledge of the way Raft is supposed to work, we found places in
the execution that deviate from our understanding of correct behavior.
We then examined the code to understand why it deviated, and came up
with a fix. Finally, we replayed to verify the bug fix.

The akka-raft case studies in Table~\ref{dtab:case_studies} are shown in the
order that we found or reproduced them. To prevent bug causes from interfering
with
each other, we fixed all other known bugs for each case study. We reported all bugs and fixes to the akka-raft
developers.

\noindent{\textbf{raft-45: Candidates accept duplicate votes from the same
election term.}} Raft is specified as a state machine with three
 states: Follower, Candidate, and Leader. Candidates attempt to
 get themselves elected as leader by soliciting a quorum of
 votes from their peers in a given election term (epoch).

In one of our early fuzz runs, we found a violation of `Leader Safety',
 i.e. two processes believed they were leader in the same election term.
 This is a highly problematic situation for Raft to be in, since the leaders may overwrite each
 others' log entries, thereby violating the key linearizability guarantee that
 Raft is supposed to provide.

The root cause for this bug was that akka-raft's candidate
 state did not detect duplicate votes from the same follower
 in the same election term. (A follower might resend votes because it
 believed that an earlier vote was dropped by the network).
 Upon receiving the duplicate vote, the candidate counts it as a new vote and steps
 up to leader before it actually achieved a quorum of votes.

\noindent{\textbf{raft-46: Processes neglect to ignore certain
 votes from previous terms.}} After fixing the previous bug, we found another
 execution where two leaders were elected in the same term.

In Raft, processes attach an `election term' number to all messages they send.
 Receiving processes are supposed to ignore any messages that contain an election
 term that is lower than what they believe is the current term.

akka-raft properly ignored lagging term numbers for some, but not all
 message types. \sys~delayed the delivery of messages from previous terms and
 uncovered a case where a candidate incorrectly accepted a vote message from a previous election term.

\noindent{\textbf{raft-56: Nodes forget who they voted for.}} akka-raft is written as a finite state machine.
When making a state transition, FSM processes specify both which state they want to transition to, and which instance variables they want to keep once they have transitioned.

All of the state transitions for akka-raft were correct except
one: when the Candidate
steps down to Follower (e.g., because it receives an `AppendEntries' message,
indicating that there is another leader in the cluster), it
{\em forgets} which node it previously voted for in that term. Now, if another
node requests a vote from it in the same term, it may vote for
a different node than it previously voted for in the same term, later causing
two leaders to be elected, i.e. a violation of Raft's ``Leader Safety''
condition. We discovered this by manually examining the state
transitions made by each process throughout the minimized execution.

\noindent{\textbf{raft-58a: Pending client commands delivered
 before initialization occurs.}} After ironing out leader election issues, we started finding other issues.
 In one of our fuzz runs, we found that a leader process threw an assertion error.

When an akka-raft Candidate first makes the state transition to leader, it does not immediately initialize its
 state (the `nextIndex' and `matchIndex' variables). It instead sends a message to itself, and initializes its state when it receives that self-message.

Through fuzz testing, we found that it is possible that the Candidate could have pending
 ClientCommand messages in its mailbox, placed there before the Candidate transitioned to Leader
 and sent itself the initialization message. Once in the Leader state, the Akka runtime will first deliver
 the ClientCommand message. Upon processing the ClientCommand message the Leader tries to replicate it to
 the rest of the cluster, and updates its nextIndex hashmap. Next, when the Akka runtime delivers the
 initialization self-message, it will overwrite the value of nextIndex. When it reads from nextIndex later,
 it is possible for it to throw an assertion error because the nextIndex values are inconsistent with
 the contents of the Leader's log.


\noindent{\textbf{raft-58b: Ambiguous log indexing.}} In one of our fuzz tests,
 we found a case where the `Log Matching' invariant was violated, i.e. log entries did not appear in the same order on all machines.

According to the Raft paper, followers should reject AppendEntries requests from leaders that are behind,
 i.e. prevLogIndex and prevLogTerm for the AppendEntries message are behind what the follower has in its log.
 The leader should continue decrementing its nextIndex hashmap until the followers stop rejecting its AppendEntries attempts.

This should have happened in akka-raft too, except for one hiccup: akka-raft decided to adopt 0-indexed logs,
 rather than 1-indexed logs as the paper suggests. This creates a problem: the initial value of prevLogIndex is ambiguous:
Followers can not distinguish between an AppendEntries for an empty log (prevLogIndex == 0)
an AppendEntries for the leader's 1st command (prevLogIndex == 0), and
an AppendEntries for the leader's 2nd command (prevLogIndex == 1 – 1 == 0).
The last two cases need to be distinguishable. Otherwise followers will not be able to reject inconsistent logs. This corner would have been hard to anticipate; at first glance it seems fine to adopt the convention that logs should be 0-indexed instead of 1-indexed.

As a result of this ambiguity, followers were unable to correctly reject AppendEntries requests from leader that were behind.

\noindent{\textbf{raft-42: Quorum computed incorrectly.}} We also found a fuzz test that
 ended in a violation of the `Leader Completeness' invariant, i.e. a newly elected leader
 had a log that was irrecoverably inconsistent with the logs of previous leaders.

Leaders are supposed to commit log entries to their state machine when they knows that
 a quorum (N/2+1) of the processes in the cluster have that entry replicated in their logs.
 akka-raft had a bug where it computed the highest replicated log index incorrectly.
 First it sorted the values of matchIndex (which denote the highest log entry index known
 to be replicated on each peer). But rather than computing the median (or more specifically,
 the N/2+1'st) of the sorted entries, it computed the mode of the sorted entries.
 This caused the leader to commit entries too early, before a quorum actually had that entry
 replicated. In our fuzz test, message delays allowed another leader to become elected, but
 it did not have all committed entries in its log due to the previously leader committing too soon.

As we walked through the minimized execution, it
became clear mid-way through the execution that not all
entries were fully replicated when the master committed
its first entry. Another process without all replicated entries
then became leader, which constituted a violation
of the ``Leader Completeness'' invariant.

\noindent{\textbf{raft-66: Followers unnecessarily overwrite log entries.}} The last issue we found is only possible to
 trigger if the underlying transport protocol is UDP, since it requires reorderings of messages between the same source, destination pair.
 The akka-raft developers say they do not currently support UDP, but they
 would like to adopt UDP in the future due to its lower latency.

The invariant violation here was a violation of the `Leader Completeness' safety property,
 where a leader is elected that does not have all of the needed log entries.

Leaders replicate uncommitted ClientCommands to the rest of the cluster in batches. Suppose a follower with an empty log receives an AppendEntries containing two entries. The follower appends these to its log.

Then the follower subsequently receives an AppendEntries containing only the first of the previous two entries (this message was delayed). The follower will inadvertently delete the second entry from its log.

This is not just a performance issue: after receiving an ACK from the follower, the leader is under the
 impression that the follower has two entries in its log.
 The leader may have decided to commit both entries if a quorum was achieved.
 If another leader becomes elected, it will not necessarily have both
 committed entries in its log as it should, leading to a `LeaderCompleteness'
 violation.


\subsection{Full Description of Spark Case Studies}
\label{app:spark_case_studies}

Spark is a large scale data analytics framework. We focused our efforts on
reproducing known bugs in the core Spark engine, which is responsible for
orchestrating computation across multiple machines.

We looked at the entire history of bugs reported for Spark's core engine. We found that most reported bugs only involve sequential
computation on a single machine (e.g. crashes due to unexpected user input).
We instead focused on reported bugs involving concurrency across machines or
partial failures. Of the several dozen reported concurrency or partial failure
bugs, we chose three.

The external events we inject for Spark case studies are worker join events
(where worker nodes join the cluster and register themselves with the master),
job submissions, and crash-recoveries of the master node. The Spark job we ran
for all case studies was a simple parallel approximation of the digits of Pi.


%  - Note on Spark's message complexity:
%      https://docs.google.com/document/d/1xlDmaACBTCkDioydjUnpQzr6j-2NQPl-3Zj_w23stSg/edit

% ------------------- spark-2294 -------------------
\noindent{\textbf{spark-2294: Locality inversion.}} In Spark, an `executor' is
responsible for performing computation for Spark jobs. Spark jobs are assigned
`locality' preferences: the Spark scheduler is supposed to launch
`NODE\_LOCAL' tasks (where the input data for the task is located on the same
machine) before launching tasks without preferences. Tasks without locality
preferences are in turn
supposed to be launched before `speculative' tasks.

The bug for this case
study was the following: if an executor E is free, a task may be speculatively assigned to E when there are other tasks in the job
that have not been launched (at all) yet. Similarly, a task without any locality preferences may be
assigned to E when there was another `NODE\_LOCAL' task that could have been scheduled.
The root cause of this bug was an error in Spark scheduler's logic: under
certain configurations of pending Spark jobs and currently available
executors, the Spark scheduler would incorrectly invert the locality priorities.
We reproduced this bug by injecting random, concurrently running Spark jobs
(with differing locality preferences) and random worker join events.

% This happens because Spark's scheduler (TaskSchedulerImpl) calls a priority function with increasing locality levels, beginning with PROCESS_LOCAL,
% followed by NODE_LOCAL, and so on until the highest currently allowed level.
% Now, supposed NODE_LOCAL is the highest currently allowed locality level.
% The first time findTask is called, it will be called with max level PROCESS_LOCAL;
% if it cannot find any PROCESS_LOCAL tasks, it will try to schedule tasks with no locality preferences
% or speculative tasks. As a result, speculative tasks or tasks with no preferences may be scheduled
% instead of NODE_LOCAL tasks.

% ------------------- spark-3150 -------------------
\noindent{\textbf{spark-3150: Simultaneous failure causes infinite restart
loop.}} Spark's
master node supports a `Cold-Replication' mode, where it commits its state to
a database (e.g., ZooKeeper). Whenever the master node crashes, the node that
replaces it can read that information from the database to bootstrap its
knowledge of the cluster state.

To trigger this bug, the master node and the driver process need to fail
simultaneously. When the master node restarts, it tries to read its state from
the database. When the driver crashes simultaneously, the
information the master reads from the database is corrupted: some of
the pointers referencing information about the driver are null. When the
master reads this information, it dereferences a null pointer and crashes
again. After failing, the master restarts, tries to recover its state, and
crashes in an infinite cycle. The minimized execution for this bug contained
exactly these 3 external events, which made the problematic code path immediately apparent.

\noindent{\textbf{spark-9256: Delayed message causes master crash.}} We found the
following bug through fuzz testing.

As part of
initialization, Spark's client driver registers with the Master node by repeatedly sending a RegisterApplication
  message until it receives a RegisteredApplication response.
 If the RegisteredApplication response is delayed by at least as long as the
 configured timeout value (or if the network
  duplicates the RegisterApplication RPC), it is possible for the Master to receive two RegisterApplication
  messages for the same client driver.

Upon receiving the second RegisterApplication message, the master attempts to
persist information about the client driver
to disk. Since the file containing information about the client driver already exists though, the master crashes with an
IllegalStateException.

This bug is possible to trigger in production, but it will occur only very
rarely. The name of the file containing information has a second-granularity timestamp associated
with it, so it would only be possible to have a duplicate file if the second
RegisteredApplication response arrived in the same second as the first
response.

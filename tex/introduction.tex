%\colin{TODO: Admit up front that, most bugs, even in distributed systems, can
%be minimized with traditional delta debugging (e.g. unexpected configurations,
%unexpected messages, etc.). However, the bugs that are the most difficult are
%those that involve concurrency \& asynchrony.}

Even simple code can contain bugs (e.g., crashes due to unexpected input). But the developers
of distributed systems face
additional challenges, such as concurrency, asynchrony, and partial failure, which require them to
consider all possible ways that non-determinism might manifest itself.
The number of event orderings %(e.g.,message deliveries, failure notifications)
a distributed system may encounter due to non-determinism grows exponentially with the number of
events in the execution. Bugs are therefore commonplace in distributed systems software, since developers invariably
overlook some unanticipated event orderings amid the exponential number of
orderings they need to consider.

% TODO(cs): simpler version of the above paragraph:
% Developers
% of distributed systems need to consider many scenarios in which the components of the system must operate,
% due to concurrency, asynchrony, and partial failures. Since analyzing these scenarios is so complex,
% it is common for distributed systems to have hard-to find bugs.

% Developers of distributed systems face notoriously
% difficult challenges, such
% as concurrency, asynchrony, and
% partial failure. These challenges, in {\em addition} to those arising from normal software development, invariably lead to software bugs.

% Bugs that are specific to distributed systems (as opposed to bugs normally encountered in sequential code, e.g. crashes due to unexpected user input) arise because
% it is difficult for developers to
% consider all the possible ways that non-determinism can manifest itself.
% The number of event orderings (e.g. message deliveries, failure notifications, etc.)
% a distributed system may encounter scales factorially with the cluster size and the execution length.
% % TODO(cs): swap order of "event-orderings" and "non-determinism"?

Software developers discover bugs in several ways.
Most commonly, they find them through unit and integration tests. These tests are ubiquitous, but they are limited to cases that developers
anticipate themselves. To uncover unanticipated cases, semi-automated testing
techniques such as randomized concurrency testing (where
sequences of message deliveries, failures, etc. are injected randomly into the system) are effective.
Finally, despite pre-release testing, bugs may turn up once the code is deployed in production.
% TODO(cs): provide citations on bugs found in production, or bugs found by fuzzing?

The last two means of bug discovery present a significant challenge to
developers: the system can run for
long periods before problems manifest themselves. The resulting executions
can contain a large number of events, most of which are not relevant to triggering the
bug. Understanding how a trace containing thousands of concurrent events lead
the system to an unsafe state requires significant expertise, time,\footnote{Developers spend a significant portion of their time
debugging (49\% of their time according to one
study~\cite{LaToza:2006:MMM:1134285.1134355}), especially when the bugs
involve concurrency (70\% of reported concurrency bugs
in~\cite{msoft_concurrency} took days to months to fix).}
and luck.

% Mention somewhere that bugs in distributed systems are important to fix?

Faulty execution traces can be made %substantially
easier to understand if they are first
{\em reduced}, so that only events that are relevant to triggering the bug remain.
In fact, software developers often start troubleshooting by manually performing this
reduction; they identify
(i) which events in the execution caused their system to arrive at the unsafe
state, and (ii) which events are irrelevant (and can be ignored).

Since developer time is typically much more costly than machine time,
automated reduction tools for {\em sequential}
test
cases~\cite{claessen2000quickcheck,Zeller:2002:SIF:506201.506206,yang2011finding}
have already proven themselves valuable,
and are routinely applied to bug reports for software projects such as Firefox~\cite{firefox_reduction}, LLVM~\cite{bugpoint}, and GCC~\cite{gcc_reduction}.
%\george{any cites from distributed systems?}
In this dissertation, we argue the following:

\begin{quote}
{\textbf{Thesis statement:}} \textit{It is possible to automatically reduce faulty
executions of distributed systems.}
\end{quote}

\noindent We primarily
focus on faulty executions generated in an automated test environment. However, we also
illustrate how one might reduce production executions.

We seek to provide execution reduction tools that leave developers of
distributed systems with a `minimal causal sequence' of triggering events,
which are necessary and sufficient for triggering the bug.
We claim that the greatly reduced size of the trace makes it easier for the developer to figure out which code path contains the underlying bug, allowing them to focus their effort on the task of fixing the problematic code itself.
As far as we know, we are the first to show how to provide execution
reduction tools
for distributed and concurrent systems without needing to analyze the code.

%Other than our prior work, the closest approaches we know of work by
% symbolically analyzing the program, which suffer from a different set of
% limitations.

\section{Challenges}

Distributed executions have two distinguishing features. Most importantly, input events
(e.g., failures)
are {\em interleaved} with internal events (e.g., intra-process
message deliveries) of concurrent processes. Execution reduction algorithms must therefore consider both which
input events and which (of the exponentially many)
event interleavings (``schedules'') are likely to be necessary and sufficient for triggering the bug. Our main contribution
throughout this dissertation is a set of techniques for searching
through the space of event schedules in a timely manner; these techniques are inspired
by our understanding of how practical systems behave.

Distributed systems also frequently exhibit non-determinism (e.g., since they
make extensive use of wall-clock timers to detect failures). To reduce executions
consistently and reliably, we need to find ways to cope with this non-determinism.

We address these challenges from two perspectives.

\subsection{Execution Reduction without Application-Layer Interposition}

In chapter~\ref{sec:sts}, we pose
the following question:

\begin{quote}
\textit{Can we automatically reduce
executions without making any assumptions about the language or
instrumentation of the software under test?}
\end{quote}

The key advantage of this `blackbox' approach to execution reduction is that
it allows us to avoid the (often substantial) engineering effort required to
instrument and completely control all the sources of non-determinism in
distributed
systems. A blackbox execution reduction tool can be applied easily to a wide
range of systems without much effort.

Rather than analyzing the code (which would violate our blackbox requirement),
we \textit{experiment} with different executions and observe how the system
behaves as a result. If we find an execution that is shorter than the
original and still triggers the same bug, we can then ignore any of the
events that we did not include from the original execution. We continue
experimenting with shorter executions until we know that we cannot make
further progress on reducing the size of the execution.

We develop our blackbox execution reduction techniques
in the context of software-defined
networking (SDN) control software (explained in~\S\ref{sec:sdn}). In lieu of
application-layer interposition, we interpose on a standard
protocol used by all SDN control software: OpenFlow~\cite{openflow}.
%With
%interposition on OpenFlow messages, we show that we are able to mitigate
%non-determinism and control the
%interleavings of events well enough to achieve decent reduction of execution
%size.

We develop an intuitive heuristic for
choosing which event interleavings to explore: we know that the original execution triggered the bug, so, when exploring modified executions we should try to stay as close as possible to the original execution's causal dependencies.
The mechanism we develop to implement this heuristic involves dynamically inferring whether events we expect to show up (from the original execution) will in fact show up in modified executions.

Without perfect interposition, we need to find ways to mitigate the effects of
non-determinism while we experimentally explore event interleavings.
Through case studies, we discover an effective set of mitigation strategies involving careful wall-clock spacing between events, and replaying non-deterministic executions multiple times.

\subsection{Execution Reduction with Application-Layer Interposition}

Our investigation in chapter~\ref{sec:sts} focuses narrowly on one kind of
distributed system, and, without complete control over the execution, the heuristics we
develop there leave room for improvement. In chapter~\ref{sec:demi},
we identify a computational model that allows us to cleanly reason
about new reduction heuristics. We also apply these heuristics successfully
to several types of distributed systems besides SDN control software.

The computational model we identify---the actor model---encapsulates all sources of
non-determinism from the network in a small number of interposition points.
This makes it easy for us to gain (nearly) perfect control over the execution of events in actor-based distributed systems.
With this control in place, it is theoretically possible to simply enumerate all possible schedules, execute each one, and pick the smallest execution that triggers the bug.
The space of all possible schedules is intractably large however, which leads to our main question:

\begin{quote}
\textit{How can we maximize reduction of trace size within bounded time?}
\end{quote}

Our approach is to carefully prioritize the order in which we explore
the schedule space. We prioritize schedules that are shorter than the
original, and that we believe will be likely to
retrigger the invariant violation (since retriggering the invariant violation allows us to make
progress on reducing the execution).

For any prioritization function we choose, an adversary could construct
the program under test, or even just the initial execution, so that our our prioritization
function does not make any reduction progress within a limited time budget.

Fortunately, the systems we care about in practice are not constructed by
adversaries. They exhibit certain \textit{program properties}, or constraints
on their computational structure,
that we can take advantage of when designing our prioritization functions.
We choose our
prioritization functions based on our understanding of these program
properties.

Drawing from what we learned in
chapter~\ref{sec:sts}, the main program property we leverage states that if one schedule triggers a bug,
schedules that are ``similar'' in their causal structure should have a high probability of also triggering the bug.
Translating this intuition into a prioritization function requires us to address our second challenge:

\begin{quote}
\textit{How can we reason about the similarity or dissimilarity of two distinct executions?}
\end{quote}

We develop a hierarchy of equivalence relations between events, and show
that systematically exploring schedules that are close to the original
execution yield significant gains in reduction over the previous
heuristics described in chapter~\ref{sec:sts}. We implement these heuristics
and use them to find and reduce several bugs in two very different types of distributed
systems: the Raft consensus protocol
and the Spark data analytics engine.

\section{Outline and Previously Published Work}

The remainder of this dissertation proceeds as follows. We provide background
on the existing test case reduction algorithms we build upon in
chapter~\ref{sec:background_material}. In chapter~\ref{sec:sts} we design and
evaluate execution reduction strategies that do not assume application-layer
interposition. In chapter~\ref{sec:demi} we identify a deterministic (yet
practically relevant)
computational model for distributed systems, and show how to effectively
reduce executions of systems that adhere to that more refined model. We discuss how both of
these approaches relate to previous literature in
chapter~\ref{sec:related_work}, and we comment on avenues for
future work and conclude in chapter~\ref{main_sec:conclusion}.

Chapter~\ref{sec:sts} revises published material from~\cite{sts2014}.
Chapter~\ref{sec:demi} revises published material from~\cite{Scott:2015:DEMi}.

%Impact: STS has gained adoption by both academics and industrial organizations: it has been used as part of the development process for the ONOS open source SDN controller; I sent an undergraduate on an internship to Big Switch Networks to integrate STS; and STS has been used as the basis for research projects at ETH and Duke [12,13].

%Impact: DEMi has already gained notable attention from industry; it has been covered by industrial bloggers other than myself, and I have received 3 invitations to speak about DEMi at industrial conferences venues.

% Delta from related work (also see: STS FAQ):
%
% traditional minimization (DDmin, QuickCheck’s shrinking)
%   not immediately applicable to systems with intermediate external events
% best-effort minimization (QuickCheck applied to concurrent systems, field failures)
%   don’t systematically handle concurrency
% model checking minimization: MAX-Sat, interpolation (ConcBugAssist, DSPs)
%   many disadvantages of model checking. See STS FAQ.
% deterministic replay log minimization (schedule minimization, program analysis)
%   don't allow divergence during replay! i.e. minimization specification is overly specific -> minimization results aren’t great
%   program flow analysis work is tied to a single language, + high perf overhead
% model inference, log summarization (synoptic, csight, invariant mining)
%   summarize the events that occurred, but don’t actually minimize the test case. Apply this after you minimize the test case with our techniques.
% program slicing, automated debugging (experimental, program analysis)
%   minimize program statements, but not the test case itself. Useful for debugging, but not troubleshooting. Apply this after you minimize the test case with our techniques.

% Our primary contribution, schedule exploration strategies,
% made it possible to apply input minimization algorithms
% %(cf. Delta
% %Debugging~\cite{Zeller:2002:SIF:506201.506206} and domain-specific
% %algorithms~\cite{claessen2000quickcheck,regehr2012test,whitaker2004configuration})
% to blackbox distributed systems.

% TODO(cs): discuss symbolic execution, NICE? (As part of "model checking")

We start our discussion of related work with the most closely
related literature.

\section{Reduction Techniques}

\noindent{\textbf{Input Reduction for Sequential Programs.}} Reduction
algorithms for sequentially processed inputs are
well-studied~\cite{Zeller:2002:SIF:506201.506206,claessen2000quickcheck,regehr2012test,whitaker2004configuration,burger2011minimizing,fse_web_ddmin,chang2007simulation}.
These (particularly: delta debugging) form a component of our solution, but they do not consider
interleavings of internal events from concurrent processes.

\noindent{\textbf{Reduction without Interposition.}} Several
tools reduce inputs to concurrent
systems without controlling sources of non-determinism~\cite{arts2006testing,clause2007technique,tucek2007triage,jin2013f3,hughes2011testing}.
The most sophisticated of these replay each subsequence
multiple times and check whether the violation is reproduced at least once~\cite{hughes2011testing,claessen2009finding}.
Their major advantage is that they avoid the engineering effort required to
interpose. Their drawback, as we found in chapter~\ref{sec:sts},
 is that bugs are often not easily reproducible without
interposition. Furthermore, without interposition these techniques cannot
(directly) reduce internal events.

% TODO(cs): the QuickCheck papers all seem quite optimistic about how
% effective simply replaying multiple times is.
% We should compare against them to see to what degree systematic exploration is in fact better.

QuickCheck's PULSE controls the message delivery
schedule~\cite{claessen2009finding} and supports schedule reduction. During replay, it considers the
order messages are sent in, but not message contents.
When it cannot replay a step, it skips it (similar to
STSSched), and reverts to random scheduling once expected messages
are exhausted~\cite{hughes_email}. As the QuickCheck developers point
out, even with PULSE's interposition, QuickCheck's reduced counterexamples can be
large~\cite{pulse_counterexamples}. With more complete exploration of the
schedule space their reduction would likely be improved.

\noindent{\textbf{Thread Schedule Reduction.}} Other techniques seek to reduce thread
interleavings leading up to concurrency
bugs~\cite{choi2002isolating,jalbert2010trace,huang2011efficient,el2014efficient}.
These generally work by iteratively feeding a single input (mutations of a
recorded thread schedule) to a single entity (a deterministic thread scheduler). These approaches
ensure that the program never diverges from the control flow of the
original schedule (otherwise the recorded I/O responses and context switch points
from the original execution would become useless). Besides reducing context
switches, these approaches at
best {\em truncate} thread executions by having threads exit earlier than they did in the original
execution (thereby shortening the execution trace), but they cannot remove extraneous events from
the middle of the trace.

Although these thread schedule reduction techniques do not
explore divergent schedules,\footnote{PRES
explores divergent schedules for best-effort replay of multithreaded
executions, but does not reduce executions~\cite{park2009pres}.}
 that constraint is not fundamental. Rather than
only considering the original recorded thread schedule, one
could explore new (divergent) schedules, store the newly recorded
I/O responses and context switch points, and replay those recordings
deterministically later on. Such a system
for multicore schedule reduction would be equivalent to \sys~as we apply it to message passing
distributed systems.

% To deterministically
% reproduce bugs, we would need visibility into every I/O request and response (\eg~clock
% values or socket reads), as well as all thread scheduling
% decisions for each process. This information is the starting point for
% techniques that seek  to minimize thread interleavings leading up to race conditions.
% These approaches involve iteratively feeding a single input (the thread
% schedule) to a single entity (a deterministic scheduler)~\cite{choi2002isolating,claessen2009finding,jalbert2010trace}, or
% statically analyzing feasible thread schedules~\cite{huang2011efficient}.
%
% A crucial constraint of these approaches is that they must keep the inputs
% fixed; that is, behavior must depend uniquely on the thread
% schedule. Otherwise, the nodes may take a divergent code path. If this
% occurs some processes might issue a previously unobserved I/O request, and the replayer will not
% have a recorded response; worse yet, a divergent process might deschedule
% itself at a different point than it did originally, so that the remainder of
% the recorded thread schedule is unusable to the replayer.
%
% Because they keep the inputs fixed, these approaches strive for a
% subtly different goal than
% ours: minimizing thread context switches rather than input events.
% At best, these approaches can indirectly minimize input events by truncating
% individual thread executions. That is, they can cause threads to exit early
% (thereby shortening the execution trace), but they cannot remove extraneous events from
% the middle of the trace.
%
\noindent{\textbf{Thread Schedule Reduction Augmented with Program
Analysis.}} One can remove events in the middle of a recorded deterministic
replay log by analyzing the program's control- and
dataflow dependencies and providing a proof that removing those events will not shift the
execution's context switch points~\cite{Lee:2011:TGR:1993498.1993528,tallam2007enabling,huang2012lean,cai2013lock,elyasov2013guided,wang2015fast}.
These techniques do not explore alternate code paths. Program analysis also
over-approximates
state reachability (because some conditionals are undecidable or difficult to
model; e.g., EFF takes the transitive closure of all possible
dependencies~\cite{Lee:2011:TGR:1993498.1993528}), disallowing them from removing dependencies that actually commute.

We compare against thread schedule reduction (without divergence) by
configuring \sys~to reduce as before,
but to abort any execution
where it detects a previously unobserved state transition. Column `NoDiverge'
of Table~\ref{dtab:case_studies} shows the results, which demonstrate that
divergent executions are crucial to \sys's reduction gains for the akka-raft case studies.

Although previous program analysis papers did not consider
exploring alternate code
paths~\cite{Lee:2011:TGR:1993498.1993528,tallam2007enabling,huang2012lean,cai2013lock,elyasov2013guided,wang2015fast},
program analysis could be leveraged to help prune \sys's search space. We plan
to investigate this idea in future work.

% %\noindent{\textbf{Program Flow Analysis.}} Other than delta debugging,
% %the closest work to ours involves reasoning about
% %control- and dataflow dependencies
% %(which may be recorded at runtime~\cite{Lee:2011:TGR:1993498.1993528},
% %or dynamically inferred~\cite{, tallam2007enabling})
% %in order to reduce the length of deterministic replay executions.
% %%decision processes of control software. % Our Peek() algorithm is a
% %% contribution in how to infer dependencies without access to software internals
% %Moreover, our application of functional equivalence to the space of
% %possible inputs allows us to minimize
% %inputs more aggressively, whereas they are forced to consider all controlflow
% %and dataflow dependencies in the control software.
%
% With additional information obtained by program flow
% analysis~\cite{Lee:2011:TGR:1993498.1993528,tallam2007enabling,huang2012lean}
% however, the inputs no longer need to be fixed. The internal events considered by these program flow reduction
% techniques are individual instructions executed by the
% programs (obtained by instrumenting the language runtime), in addition to I/O responses and the thread schedule.
% With this information they can compute
% program flow dependencies, and thereby remove input events from anywhere in the trace as long as they
% can prove that doing so cannot possibly cause the faulty execution path to diverge.
%
% While program flow reduction is able to minimize inputs,
% these techniques are not able to explore alternate code paths that still
% trigger the invariant violation. They are also overly conservative in
% removing inputs (\eg~EFF takes the transitive closure of all possible
% dependencies~\cite{Lee:2011:TGR:1993498.1993528}) causing them to miss opportunities to
% remove dependencies that actually semantically commute.
%
% We also avoid the performance overhead of recording all I/O
% requests and later replaying them (\eg~EFF incurs \textasciitilde10x slowdown during
% replay~\cite{Lee:2011:TGR:1993498.1993528}). Lastly,
% we avoid the extensive effort required to instrument the control software's language runtime,
% needed by the other approaches to implement a deterministic thread scheduler, interpose on syscalls,
% or perform program flow analysis. By avoiding assumptions about the language of the control software,
% we were able to easily apply our system to five different control platforms
% written in three different languages.

\noindent{\textbf{Model Checking.}} The algorithmic aspects of this
dissertation are most closely
related to the model checking literature.

Abstract model checkers convert (possibly concurrent) programs to logical formulas, find
logical contradictions (assertion violations) using solvers, and reduce the logical
conjunctions to aid
understanding~\cite{christ2013flow,khoshnood2015concbugassist,machado2015concurrency}.
Model checkers are very powerful, but they are typically tied to a single
language and assume access to source code, whereas the systems we target (e.g.
Spark) are composed of multiple languages and may use proprietary
libraries.

It is also possible to extract logic formulas from raw binaries~\cite{avgerinos2014enhancing}.
Fuzz testing is significantly lighter weight.

If, rather than randomly fuzzing, testers
enumerated inputs of progressively larger sizes, failing tests would
be minimal by construction. However, breadth first enumeration takes very long to get to
`interesting' inputs (After 24 hours of execution, our bounded DPOR
implementation with depth bound slightly greater than the optimal trace size still had not found any invariant violations. In
contrast, \sys's randomized testing discovered
most of our reported bugs within 10s of minutes).
Furthermore, reduction is
useful beyond testing, e.g. for simplifying production traces.

Motivated by the intractability of systematic input enumeration, many papers
develop heuristics for finding bugs
quickly~\cite{tzoref2007instrumenting,musuvathi2007iterative,musuvathi2008finding,yabandeh2009crystalball,burckhardt2010randomized,terragnirecontest,fonseca2014ski,leesatapornwongsa2014samc,lin2009modist,park2009ctrigger,coons2010gambit}.
We do the same, but crucially, we are able to use
information from previously failing executions to guide our search.
We are also the first to observe that
an application's use
of TCP constrains its schedule space.

As far as we know, we are the first to combine DPOR and delta debugging to
reduce executions. Others have modified DPOR to keep state~\cite{yang2008efficient,yi2006stateful}
and to apply heuristics for choosing initial schedules~\cite{lauterburg2010evaluating}, but these
changes are intended to help find new bugs rather than reduce existing
faulty executions.

\noindent{\textbf{Program Slicing \& Automated Debugging.}} Program slicing~\cite{weiser1981program}
seeks to find a minimal subset of the statements in a program that could
possibly affect the outcome of an assertion.
Literature following Weisers' original program
slicing paper goes further to try to
automatically locate the exact line(s) of code or state transitions that are responsible for a
bug, using statistical data~\cite{zhangzhang}, test coverage
data~\cite{coverage_localization,xuan14}, constraint solving~\cite{jose11},
fault injection~\cite{zhang13} and
experimentally driven executions of the failing program~\cite{zeller2005,comparative_causality}.
Our goal is to slice the temporal dimension of an execution rather than the
code dimension.

\noindent{\textbf{Log Comprehension.}} Model inference techniques summarize log files
in order to make them more easily understandable by
humans~\cite{ernst2001dynamically,synoptic,csight,biermann1972synthesis,lorenzoli2008automatic,lou2010mining}.
Model inference is complementary, as it does not modify the event logs.

\section{Debugging for Networked \& Concurrent Systems}

We end this chapter by discussing
literature on the general topic of troubleshooting. For an in-depth overview of
systematic troubleshooting techniques, see our HotSDN publication~\cite{heller2013leveraging}.

% TODO(cs): include this preface?:
% We characterize the other troubleshooting approaches
% as (i) instrumentation (tracing),
% (ii) bug detection (invariant checking),
% (iii) replay,
% (iv) root cause analysis (of device failures), (v) log comprehension
% (visualization), (vi) program slicing, and (vii) automated debugging (fault
% localization).\\[0.5ex]

\noindent{\textbf{Interposition on blackbox distributed systems.}} \sys's
software architecture closely resembles other automated testing systems for
distributed
systems~\cite{lin2009modist,leesatapornwongsa2014samc,simsa2010dbug}. All of
these tools interpose on timers and message send and receive events. MoDist~\cite{lin2009modist}
interposes at the syscall level, whereas SAMC~\cite{leesatapornwongsa2014samc} and dBug~\cite{simsa2010dbug}
interpose primarily at the application layer (similar to \sys). As we discussed in \S\ref{subsec:generality}, interposing
at the application layer has several advantages over interposing at the
syscall level, although syscall level interposition is more broadly applicable.

\projectname's software architecture closely resembles other network
simulators~\cite{handigol2012reproducible,ns-3,Vahdat:2002:SAL:844128.844154}.
Most of these simulators are focused on producing high fidelity performance
behaviors, whereas we designed \projectname~with the intent of providing
precise control over event timings without focusing on performance.

\noindent{\textbf{Root Cause Analysis.}} Without perfect instrumentation,
it is often not possible to know exactly what events are occurring (\eg~which
 components have failed) in a
 distributed system. Root cause analysis~\cite{yemini1996,Kandula:2009:DDE:1592568.1592597}
 seeks to reconstruct those unknown events from limited monitoring data.
 Here we know exactly which events occurred, and instead
 seek to identify a minimal sequence of events.

\noindent{\textbf{Bug Reproduction.}} Record and replay
techniques seek to reproduce a given concurrency
bug~\cite{zamfir2011debug,Zamfir:2010:EST:1755913.1755946,altekar2009odr,park2009pres,Yuan:2010:SED:1736020.1736038,ofrewind}.
Once a bug has been
reproduced, users can step through the (deterministic) execution and interactively examine the
state of the system.
%Closely related to \projectname, OFRewind~\cite{} provides
%record and replay of OpenFlow channels between controllers and switches.
Manually examining long system executions can be tedious, and our goal is to
reduce such executions so that developers find it easier to identify the
problematic code through replay or other means.

\noindent{\textbf{Probabilistic Diagnosis.}} Record and replay systems incur
performance overheads at runtime that can be too prohibitive for production deployments.
With the aim of avoiding the runtime overhead of deterministic replay, probabilistic diagnosis
techniques~\cite{Yuan:2010:SED:1736020.1736038,sangmin,kasikci2015failure}
capture carefully selected diagnostic information (e.g. stack traces,
thread \& message interleavings) that should have high
probability of helping developers find the root cause of a problem. One key insight
underlying these techniques is cooperative debugging: the realization that
even if one does not collect enough diagnostic information from a single bug report,
it is quite likely that the bug will happen more than once so that diagnostic
information can be correlated~\cite{coop_debugging}.
We assume more complete runtime instrumentation (during testing, not in production), but provide exact reproducing scenarios.

\noindent{\textbf{Detecting Bugs in Production.}} Despite our best efforts, bugs invariably make it into production. Still,
we would prefer to discover these issues through means that are more
proactive than user complaints. An old idea is useful here: distributed
snapshots~\cite{Chandy:1985:DSD:214451.214456},
a subset of the events in
the system's execution such if any event e is contained in the subset, all
`happens-before' predecessors of e are also contained in the subset.
Distributed snapshots allow us to obtain a global view of the state of all
machines in the system, without needing to stop the world. Once we have a
distributed snapshot in hand, we can check assertions about the state of the
overall system (either offline~\cite{Liu07widschecker} or online~\cite{d3s}).
Within the networking community, research along these lines has focused on
verifying routing tables~\cite{hsa,hsa_realtime,anteater,khurshid2012veriflow}
or forwarding behavior~\cite{Zeng:2012:ATP:2413176.2413205,libra}. We focus on
reducing executions leading up to bugs, assuming we already have access to
some mechanism for detecting those bugs.


%With instrumentation available, it becomes possible
%to check expectations about the
%system's state (either offline~\cite{Liu07widschecker} or online~\cite{d3s}), or about the paths requests take through
%the system~\cite{pip}.

% ### Regression Testing for Performance (Latency) Bugs
%
% The regression testing problem for latency bugs is similar to above, with a
% few differences:
%
%  - **We're given**: (i) an assertion that we know the system has
%    violated in the past, usually statistical in nature, about how long requests should take to be
%    processed by the system, and (ii) a description of the system's workload
%    at the time the latency problem was detected.
%  - **Our goal**: we want to produce an oracle that will notify us
%    whenever request latency gets notably worse as we make new changes to the
%    system.
%
% A few challenges:
%
%  - **a) Flakiness**: Performance characteristics typically exhibit large
%    variance. Despite variance, we need our assertion to avoid reporting too
%    many false positives. Conversely, we need to prevent the assertion from
%    missing too many true positives.
%  - **b) Workload characterization**: it can be difficult to reproduce production
%    traffic mixes in a test environment.
%
% Regardless of whether your system is distributed or located on a single machine,
% request latency is defined by the time it takes to execute the program's *control flow* for that request.
% In a system where concurrent tasks process a single request, it is useful to
% consider the *critical path*: the longest chain of dependent tasks
% starting with the request's arrival, and ending with completion of the control
% flow.
%
% The challenges with observing control flow for distributed systems are the
% following:
%
%   - **c) Limited Visibility**: the control flow for a single request can touch
%     thousands of machines, any one of which might be the source of a latency
%     problem.[^16] So, we need to aggregate timing information across
%     machines. Simple aggregation of statistics often isn't sufficient though, since a
%     single machine doesn't have a way of knowing which local tasks were
%     triggered by which incoming request.
%   - **d) Instrumentation Overhead**: It's possible that the act of measuring execution times can
%     itself significantly perturb the execution time, leading to false positives or false negatives.
%   - **e) Intrusiveness**: if we're using our production deployment to find performance problems, we need to
%     avoid increasing latency too much for clients.
%
% #### Solutions
%
% The main technique for addressing these challenges is **distributed tracing**. The
% core idea is simple:[^25] have the first machine assign an ID to the incoming request,
% and attach that ID (plus a pointer to the parent task) to all messages that are generated in response to the
% incoming request. Then have each downstream task that is involved in processing those
% messages log timing information associated with the request ID to disk.
%
% Propagating the ID across all machines results in a *tree* of
% timing information, where each vertex contains timing information for a single
% task (the ingress being the root), and each edge represents a control flow dependency between tasks.
% This timing information
% can be retrieved asynchronously from each machine.
% To minimize instrumentation overhead and intrusiveness, we can *sample*:
% only attach an ID to a fraction of incoming requests. As long as overhead
% is low enough, we could overcome the
% challenge of workload characterization by running causal tracing on our
% production deployment.
%
% <!-- TODO: how does causal tracing overcome flakiness -->
%
% Here is an illustration[^17]:
%
% ![Trace Example](http://eecs.berkeley.edu/~rcs/research/tracing_example.png){:height="700px" width="700px"}
%
% What can we do with causal trees? A bunch of cool stuff: characterize the
% production workload so that we can reproduce it in a test environment,[^20]
% resource accounting[^21] and 'what-if' predictions for resource planning,[^22]
% track flows across administrative domains,[^23] visualize traces and express expectations about how flows should
% or should not be structured,[^24] monitor performance isolation in a multi-tenant environment,[^27] and most relevant for performance regression
% testing: detecting and diagnosing performance anomalies.[^26]
%
% Distributed tracing does require a fair amount of engineering effort: we need to
% modify our system to attach and propagate IDs (it's unfortunately non-trivial to
% 'bolt-on' a tracing system like Zipkin). Perhaps the simplest form of
% performance regression testing we can do is to analyze performance statistics
% **without correlating** across machines. We can still get end-to-end latency
% numbers by instrumenting clients, or by ensuring that
% the machine processing the incoming request is the same as the machine sending
% an acknowledgment to the client. The key issue then is figuring
% out the source of latency once we have detected a problem.

\noindent{\textbf{Instrumentation.}} Making sense of unstructured diagnostic
information pieced together from a large collection of machines is
challenging. The goal of distributed tracing frameworks is to
produce structured event logs that can be easily analyzed to understand performance and
correctness bugs. The core idea~\cite{path} is simple: have the first machine assign an ID to the incoming request,
and attach that ID (plus a pointer to the parent task) to all messages that are generated in response to the
incoming request. Then have each downstream task that is involved in processing those
messages log timing information associated with the request ID to disk.
Propagating the ID across all machines results in a tree of
timing and causality information.
%, where each vertex contains timing information for a single
%task (the ingress being the root), and each edge represents a control flow dependency between tasks.
%This timing information
%can be retrieved asynchronously from each machine.
%To minimize instrumentation overhead and intrusiveness, we can sample:
%only attach an ID to a fraction of incoming requests.
% As long as overhead
% is low enough, we could overcome the
% challenge of workload characterization by running causal tracing on our
% production deployment.

Causal trees can be used\footnote{See~\cite{tracing_overview} for a
comprehensive survey of distributed tracing tradeoffs and use-cases.} to: characterize the
production workload~\cite{barham2004using}, handle
resource accounting~\cite{stardust} and `what-if' predictions for resource
planning~\cite{ironmodel},
track flows across administrative domains~\cite{fonseca2007x}, visualize traces and express expectations about how flows should
be structured~\cite{pip}, monitor performance isolation in a multi-tenant
environment~\cite{retro}, track the flow of packets through a
network~\cite{ndb}, and detect performance anomalies~\cite{perf_changes}.
These use-cases enable developers to understand
how, when, and where the system broke. In contrast, we seek to provide the
minimal set of events needed for the software
to violate an invariant.

% It is also possible to infer
% performance anomalies by building probabilistic models from
% collections of traces~\cite{barham2004using,Chen02pinpoint:problem}.
% Our goal is to produce exact minimal causal sequences, and we are primarily focused on
% correctness instead of performance.

% ### Discovering Problems in Production
%
% Despite our best efforts, bugs invariably make it into production.[^28] Still,
% we'd prefer to discover and diagnose these issues through means that are more
% proactive than user complaints.
% What are the challenges of detecting problems in production?:
%
%  - **a) Runtime overhead**: It's crucial that our instrumentation doesn't incur noticeable
%    latency costs for users.
%  - **b) Possible privacy concerns**: In some cases, our monitoring data
%    will contain sensitive user information.
%  - **c) Limited visibility**: We can't just stop the world to collect
%    our monitoring data, and no single machine has global visibility into
%    the state of the overall system.
%  - **d) Failures in the monitoring system**: The monitoring system is itself a
%    distributed system that needs to deal with faults gracefully.
%
% #### Solutions
%
% An old idea is particularly useful here: **distributed snapshots**~\cite{Chandy:1985:DSD:214451.214456}.
% Distributed snapshots are defined by consistent cuts: a subset of the events in
% the system's execution such if any event e is contained in the subset, all
% 'happens-before' predecessors of e are also contained in the subset.
%
% Distributed snapshots allow us to obtain a global view of the state of all
% machines in the system, without needing to stop the world. Once we have a
% distributed snapshot in hand, we can check assertions about the state of the
% overall system (either offline~\cite{Liu07widschecker} or online~\cite{d3s}).

% TODO(cs): cite other topics from blog post, e.g. dynamic analysis for race condition detection.
%  - Fault tolerance testing
%  - Test case reduction
%  - Distributed debugging
%  - Tools to help you write correctness conditions
%  - Tools to help you better comprehend diagnostic information
%  - Dynamic analysis for finding race conditions & atomicity violations
%  - Model checking & symbolic execution
%  - Configuration testing
%  - Verification
%  - Liveness issues
%  - ...

% Cite

% TODO(cs): cite MoDist, as where we got our software architecture from.
%    Describes the design of a model checker that acts on *unmodified*
%    distributed systems. It does this by interposing on messages, but not
%    individual function calls as in traditional model checking. Our
%    interposition follows this design.
% TODO(cs): cite the paper on minimizing inputs to hardware:
%    A nice way to develop an intuition for what we’re trying to do: read this
%    paper. The paper has the same goal, but in a much simpler environment:
%    minimizing inputs to hardware. So, no concurrency, a single clock for the
%    entire system, complete visibility into the state of the system at any point
%    in time, well-defined inputs, somewhat tractable state space.  It does a nice
%    job of describing, in terms of state machines, what I think is going on "under
%    the hood" when we run our minimizations.
% TODO(cs): cite FSE paper on minimizing JS inputs. Also contains a
% user-study showing that minimization is useful for debugging! http://brrian.org/
% TODO(cs): cite Max-SAT papers: "flow-sensitive fault localization", and
% "ConcBugAssist: Constraint Solving for Diagnosis and Repair of Concurrency Bugs"
% TODO(cs): cite "Concurrency Debugging with Differential Schedule Projections" from PLDI 2015.
% Basically, it finds minimal differences between a passing and failing concurrent execution.
% Not clear that differences are the way developers really debug.
% TODO(cs): cite F3: fault localization for field failures. follow-on to clause2007technique.
% TODO(cs): cite "topological analysis", related most closely to "log comprehension" below.
% Basically, identify which parts of the log are relevant.
% TODO(cs): heuristics for finding schedules that trigger bugs:
%  - CHESS: http://research.microsoft.com/en-us/projects/chess/
%  - PCT [follow-on]: http://research.microsoft.com/pubs/118655/asplos277-pct.pdf
%  - SKI [Pedro's work for OS kernels]: https://www.mpi-sws.org/~pfonseca/papers/osdi2014-ski.pdf
% TODO(cs): possibly related: Recontest, Effective Regression testing of concurrent programs.
% Heuristics for finding regression failures in concurrent programs, by exploring interleavings
% that are likely to retrigger the old bug.
% TODO(cs): cite "Instrumenting where it hurts", which adds noise to the
% schedule, then applies delta debugging to minimize the noise points.
% TODO(cs): cite SAMC, Crystal Ball:
% \cite{leesatapornwongsa2014samc, yabandeh2009crystalball}


% \noindent{\bf Techniques for Selecting Subsequences.} Checking
% random subsequences of $E_L$ would be one viable but inefficient
% approach to achieving our first task. A better approach is
% %divide-and-conquer search technique from the software engineering community:
% the delta debugging algorithm~\cite{Zeller:1999:YMP:318773.318946,Zeller:2002:SIF:506201.506206}, a
% divide-and-conquer algorithm for
% isolating fault-inducing inputs. We use delta
% debugging\footnote{Domain-specific
% minimization algorithms also
% exist~\cite{regehr2012test,claessen2000quickcheck,whitaker2004configuration},
% but we focus on the general case.} to iteratively select subsequences of $E_L$ and $replay$ each
% subsequence with some timing $T$.\\[0.5ex]
% %If the bug persists for a given subsequence, delta debugging ignores the
% %other inputs, and proceeds with the search for an MCS within this subsequence.
% %In what follows, we use the term {\em delta debugging} to refer to our algorithm for finding relevant subsequences.
% %The delta debugging algorithm we implement is shown in Figure~\ref{fig:ddmin}.
% %
% \noindent{\bf Techniques for Selecting Timings.} Simply
% exploring subsequences $E_S$ of $E_L$ is insufficient for finding
% MCSes: the timing of when we inject the external events during $replay$ is crucial for
% reproducing violations.
%
% The most natural approach to scheduling
% external events is to maintain the original wall-clock timing intervals
% between them.
% If this is able to find all minimization opportunities,
% \ie~reproduce the violation for all
% subsequences that are a supersequence of some MCS, we say that the inputs are
% isolated. The original applications of delta
% debugging~\cite{Zeller:1999:YMP:318773.318946,Zeller:2002:SIF:506201.506206,regehr2012test,burger2011minimizing} make this assumption (where a
% single input is fed to a single program), as well as QuickCheck's input ``shrinking''~\cite{claessen2000quickcheck}
% when applied to blackbox systems like synchronous
% telecommunications protocols~\cite{arts2006testing}.
% \eat{\ie~if we
% define $t_i$ as the timestamp of the $i^{th}$ input from the original run and $t'_i$ as the
% replay clock value when it injects that same input
% (which may or may not be the $i$'th input in the subsequence), then we might
% just set $t'_i = t_i$.
% \eat{
% \begin{align*}
% t'_0 = t_0 \\
% t'_i = t'_{i-1} + |t_{i} - t_{i-1}|
% \end{align*}
% }
% }
%
% We tried this approach, but were rarely
% able to reproduce invariant violations. As our case studies
% demonstrate~\cite{sts2014}, this is largely due
% to the concurrent, asynchronous nature of distributed systems; consider that the network
% can reorder or delay messages, or that processes may
% respond to multiple inputs simultaneously.
% Inputs injected according to wall-clock time are not guaranteed to
% coincide correctly with the current state of the processes.
% %\eat{: when we replay only a
% %subsequence of the original inputs, the reaction of the control software
% %can change, such that it behaves differently or takes a different amount of
% %time to respond to the remaining inputs events.
% %In practice we have observed that simply maintaining relative timings can
% %result in injecting the remaining inputs too early or late.}
%
% We must therefore consider the distributed system's internal events. To deterministically
% reproduce bugs, we would need visibility into every I/O request and response (\eg~clock
% values or socket reads), as well as all thread scheduling
% decisions for each process. This information is the starting point for
% techniques that seek  to minimize thread interleavings leading up to race conditions.
% These approaches involve iteratively feeding a single input (the thread
% schedule) to a single entity (a deterministic scheduler)~\cite{choi2002isolating,claessen2009finding,jalbert2010trace}, or
% statically analyzing feasible thread schedules~\cite{huang2011efficient}.
%
% A crucial constraint of these approaches is that they must keep the inputs
% fixed; that is, behavior must depend uniquely on the thread
% schedule. Otherwise, the nodes may take a divergent code path. If this
% occurs some processes might issue a previously unobserved I/O request, and the replayer will not
% have a recorded response; worse yet, a divergent process might deschedule
% itself at a different point than it did originally, so that the remainder of
% the recorded thread schedule is unusable to the replayer.
%
% Because they keep the inputs fixed, these approaches strive for a
% subtly different goal than
% ours: minimizing thread context switches rather than input events.
% At best, these approaches can indirectly minimize input events by truncating
% individual thread executions. That is, they can cause threads to exit early
% (thereby shortening the execution trace), but they cannot remove extraneous events from
% the middle of the trace.
%
% % (\ie~causing them to exit early).
%
% %\noindent{\bf Program Flow Analysis.} Other than delta debugging,
% %the closest work to ours involves reasoning about
% %control- and dataflow dependencies
% %(which may be recorded at runtime~\cite{Lee:2011:TGR:1993498.1993528},
% %or dynamically inferred~\cite{, tallam2007enabling})
% %in order to reduce the length of deterministic replay executions.
% %%decision processes of control software. % Our Peek() algorithm is a
% %% contribution in how to infer dependencies without access to software internals
% %Moreover, our application of functional equivalence to the space of
% %possible inputs allows us to minimize
% %inputs more aggressively, whereas they are forced to consider all controlflow
% %and dataflow dependencies in the control software.
%
% With additional information obtained by program flow
% analysis~\cite{Lee:2011:TGR:1993498.1993528,tallam2007enabling,huang2012lean}
% however, the inputs no longer need to be fixed. The internal events considered by these program flow reduction
% techniques are individual instructions executed by the
% programs (obtained by instrumenting the language runtime), in addition to I/O responses and the thread schedule.
% With this information they can compute
% program flow dependencies, and thereby remove input events from anywhere in the trace as long as they
% can prove that doing so cannot possibly cause the faulty execution path to diverge.
%
% While program flow reduction is able to minimize inputs,
% these techniques are not able to explore alternate code paths that still
% trigger the invariant violation. They are also overly conservative in
% removing inputs (\eg~EFF takes the transitive closure of all possible
% dependencies~\cite{Lee:2011:TGR:1993498.1993528}) causing them to miss opportunities to
% remove dependencies that actually semantically commute.\\[0.5ex]
% %
% \noindent{\bf Our Contribution.} For the first portion of this dissertation
% proposal (\S\ref{dsec:past_work}) we
% discuss heuristics we have developed for interleaving internal and external
% events that we have empirically shown to work
% well for minimizing event traces. Our approach there is
% to allow processes to proceed along divergent paths
% rather than recording all low-level I/O and thread scheduling decisions.
% This has several advantages. Unlike
% the other approaches, we can find shorter alternate code paths that still
% trigger the invariant violation. Previous {\em best-effort} execution minimization
% techniques~\cite{clause2007technique,tucek2007triage,chang2007simulation} also allow alternate
% code paths, but do not systematically
% consider concurrency and asynchrony.\footnote{PRES
% explores alternate code paths in best-effort replay of multithreaded
% executions, but does not minimize executions~\cite{park2009pres}.}
% We also avoid the performance overhead of recording all I/O
% requests and later replaying them (\eg~EFF incurs \textasciitilde10x slowdown during
% replay~\cite{Lee:2011:TGR:1993498.1993528}). Lastly,
% we avoid the extensive effort required to instrument the control software's language runtime,
% needed by the other approaches to implement a deterministic thread scheduler, interpose on syscalls,
% or perform program flow analysis. By avoiding assumptions about the language of the control software,
% we were able to easily apply our system to five different control platforms
% written in three different languages.
%
% Unfortunately, these heuristics do not lend themselves to solid theoretical
% explanations for why they work well. In the latter portion of the
% proposal (\S\ref{dsec:future_work}) we outline a plan for
% developing scheduling strategies that are based on sound principles (starting
% with visibility into the structure of the software under test).
% As far as we know, the problem statement and approach we pose there---applying model checkers
% to {\em certify} whether each subsequence chosen by delta debugging reproduces
% the original bug---has not appeared in the literature before.\\[0.5ex]


\eat{ % ----- OLD TEXT ---------
Our work spans three fields: software engineering, programming languages, and
systems and networking.

\noindent{\textbf{Software Engineering \& Programming Languages}}
Sherlog~\cite{Yuan:2010:SED:1736020.1736038} takes on-site logs from a
single program that ended in a failure as input, and applies static analysis to infer the
program execution (both code paths and data values) that lead up to the failure.
Along a similar vein, execution
synthesis~\cite{Zamfir:2010:EST:1755913.1755946} takes a program and a bug
report as input, and employs symbolic execution to find a thread schedule that will
reproduce the failure. The authors of delta debugging
applied their technique to multi-threaded (single-core) programs
to identify the minimum set of thread
switches from a thread schedule (a single input file) that reproduces
a race condition~\cite{choi2002isolating}. Chronus presents a simpler search
algorithm than delta debugging that is specific to configuration
debugging~\cite{whitaker2004configuration}.
All of these techniques focus on troubleshooting single, non-distributed
systems.

Rx~\cite{qin2005rx} is a technique for improving availability: upon
encountering a crash, it starts from a previous checkpoint, fuzzes
the environment (\eg~random number generator seeds) to avoid triggering the same bug,
and restarts the program. Our
approach perturbs the inputs rather than the environment
prior to a failure.

Model checkers such as Mace~\cite{Killian:2007:MLS:1250734.1250755} and
NICE~\cite{nice} enumerate all possible code paths taken by control software (NOX)
and identify concrete inputs that cause
the system to enter invalid configurations. Model checking works well for small
control programs and a small number of machines, but suffers from exponential
state explosion when run on large systems. For example, NICE took 30 hours to
model check a network with two switches, two hosts, the MAC-learning
control program (98 LoC), and five concurrent
messages between the hosts~\cite{nice}. Rather than exploring all
possibilities, we discover bugs through testing and systematically
enumerate subsequences of their event traces
in polynomial time.

\noindent{\textbf{Systems and Networking}}
We share the common goal of improving troubleshooting
of software-defined networks with OFRewind~\cite{ofrewind} and
recent project ndb~\cite{handigol2012debugger}. OFRewind provides
record and replay of OpenFlow control channels, and
allows humans to manually step through and filter input traces.
We focus on testing corner cases and automatically
finding minimal input traces.

ndb provides a
trace view into the OpenFlow forwarding tables
encountered by historical and current packets in the network.
This approach is well suited for troubleshooting hardware problems, where the
network configuration is correct but the forwarding behavior is not.
In contrast, we focus on bugs in control software; our technique
automatically identifies the control plane decisions that installed
erroneous routing entries.

Neither ndb nor OFRewind address the problem of diagnostic information
overload: with millions of packets on the wire, it can
be challenging to pick just the right subset to interactively debug.
To the
best of our knowledge, \projectname~is the first system that programmatically provides
information about precisely what caused the network to enter an invalid
configuration in the first place.

\colin{
Runtime invariant checking.
WiDS checker introduced the notion of recording
production executions to be later replayed and verified in a controlled simulation~\cite{Liu07widschecker}.
D3S made this invariant checking process realtime.
}

Trace analysis frameworks such as Pip~\cite{pip} allow developers
to programmatically check whether their expectations about the structure of
recorded causal
traces hold. MagPie~\cite{barham2004using} automatically identifies anomalous
traces, as well as unlikely transitions within anomalous traces by constructing
a probabilistic state machine from a large collection of traces and
identifying low probability paths.
Our approach seeks to identify the exact minimal causal set of inputs without
depending on probabilistic models.

\colin{Cut:}
Network simulators such as
Mininet~\cite{handigol2012reproducible}, ns-3~\cite{ns3}, and ModelNet~\cite{Vahdat:2002:SAL:844128.844154}
are used to prototype and test network software.
Our focus on comparing diverged histories requires us
to provide precise replay of event sequences, which is in tension with the performance
fidelity goals of pre-existing simulators.

% Could remove dependency inference citation if strapped for space
Root cause analysis~\cite{yemini1996} and dependency inference~\cite{Kandula:2009:DDE:1592568.1592597}
techniques seek to identify the minimum set of failed
components (\eg~link failures) needed to explain a collection of alarms. Rather than
focusing on individual component failures, we seek to reduce inputs that affect the behavior
of the overall distributed system.

% Debug Determinism~\cite{zamfir2011debug}: suggests that replay debuggers should not seek to achieve
% perfect fidelity -- the utility of the replay can still be high, even if all failure modes can't be reproduced.
% Our approach follows this argument: can't catch everything, but it's still useful!

\colin{Reviewer OD: read Knowledge Plane For The
Internet~\cite{Clark:2003:KPI:863955.863957}}
} % \eat most recent version

% [^1]: "Heartbeat: A Timeout-Free Failure Detector for Quiescent Reliable Communication." International Workshop on Distributed Algorithms '97
%
% [^3]: The test framework can't modify the messages sent by the system, but it can control other sources of non-determinism, e.g. the order in which messages are delivered.
%
% [^2]: "To Infinity and Beyond: Time-Warped Network Emulation", NSDI '06
%
% [^4]: "Replay Debugging For Distributed Applications", ATC '06
%
% [^5]: "Hardware and software approaches for deterministic multi-processor replay of concurrent programs", Intel Technology Journal '09
%
% [^6]: "Execution Synthesis: A Technique For Automated Software Debugging", EuroSys '10
%
% [^7]: "ReVirt: Enabling Intrusion Analysis Through Virtual-Machine Logging and Replay", OSDI '02
%
% [^8]: "Deterministic Process Groups in dOS", OSDI '10.
%
% [^9]: "DDOS: Taming nondeterminism in distributed systems", ASPLOS '13
%
% [^10]: "Minimizing Faulty Executions of Distributed Systems", NSDI '16
%
% [^11]: "Testing a Database for Race Conditions with QuickCheck", Erlang '11
%
% [^12]: "PRES: Probabilistic Replay with Execution Sketching on Multiprocessors", SOSP '09
%
% [^15]: "ODR: Output-Deterministic Replay for Multicore Debugging", SOSP '09
%
% [^13]: "Analyzing Multicore Dumps to Facilitate Concurrency Bug Reproduction", ASPLOS '10
%
% [^14]: "Debug Determinism: The Sweet Spot for Replay-Based Debugging", HotOS '11
%
% [^16]: We might not need to aggregrate statistics from all the machines, but at the very least we need timings from the first machine to process the request and the last machine to process the request.
%
% [^17]: "So, you want to trace your distributed system? Key design insights from years of practical experience", CMU Tech Report
%
% [^18]: "DDOS: Taming nondeterminism in distributed systems", ASPLOS '13.
%
% [^19]: "Deterministic Process Groups in dOS, OSDI '10". [Technically deterministic execution, not deterministic replay]
%
% [^20]: "Using Magpie for request extraction and workload modelling", SOSP '04
%
% [^21]: "Stardust: tracking activity in a distributed storage system", SIGMETRICS '06
%
% [^22]: "Ironmodel: robust performance models in the wild",  SIGMETRICS '08
%
% [^23]: "X-trace: a pervasive network tracing framework", NSDI '07
%
% [^24]: "Pip: Detecting the Unexpected in Distributed Systems", NSDI '06
%
% [^25]: "Path-based failure and evolution management", SOSP '04
%
% [^26]: "Diagnosing performance changes by comparing request flows", NSDI '11
%
% [^27]: "Retro: Targeted Resource Management in Multi-tenant Distributed Systems", NSDI '15
%
% [^28]: 'Hark', you say! 'Verification will make bugs a thing of the past!' --I'm not entirely convinced...
%
% [^29]: Distributed Snapshots: Determining Global States of Distributed Systems, ACM TOCS '85
%
% [^30]: WiDS Checker: Combating Bugs in Distributed Systems, NSDI '07
%
% [^31]: D3S: Debugging Deployed Distributed Systems, NSDI '08
%
% [^32]: SherLog: Error Diagnosis by Connecting Clues from Run-time Logs, ASPLOS '10
%
% [^33]: Effective Fault Localization Techniques for Concurrent Software, PhD Thesis
%
% [^34]: Failure Sketching: A Technique for Automated Root Cause Diagnosis of In-Production Failures, SOSP '15
%
% [^bofa]: Cooperative Bug Isolation, PhD Thesis
%
% [^36]: A Survey of Fault Localization Techniques in Computer Networks, SCP '05
%
% [^37]: Detailed Diagnosis in Enterprise Networks, SIGCOMM '09
